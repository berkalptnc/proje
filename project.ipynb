{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1co2Et1EzhfRTwMsMCtX51LqFqvW0Fx1v",
      "authorship_tag": "ABX9TyNPmdl/whu0CtrSClJWBav6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/berkalptnc/proje/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nwA0et15cuoh"
      },
      "outputs": [],
      "source": [
        "# Pkgs loading\n",
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdJkoCKCc3Ev",
        "outputId": "07bb4fa3-2988-4b3b-b269-e2acac70844e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_dataset_path='/content/drive/MyDrive/Proje/UrbanSound8K/UrbanSound8K/audio'\n",
        "metadata=pd.read_csv('/content/drive/MyDrive/Proje/UrbanSound8K/UrbanSound8K/metadata/UrbanSound8K.csv')\n",
        "metadata.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Mb0bAbnpdJj5",
        "outputId": "27c9c0c2-5b25-4bd7-b05a-7bd63ac8d936"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
              "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
              "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
              "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
              "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
              "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
              "\n",
              "              class  \n",
              "0          dog_bark  \n",
              "1  children_playing  \n",
              "2  children_playing  \n",
              "3  children_playing  \n",
              "4  children_playing  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f213291a-1e74-49a9-b553-324df3ff33c6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>slice_file_name</th>\n",
              "      <th>fsID</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>salience</th>\n",
              "      <th>fold</th>\n",
              "      <th>classID</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100032-3-0-0.wav</td>\n",
              "      <td>100032</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.317551</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>dog_bark</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100263-2-0-117.wav</td>\n",
              "      <td>100263</td>\n",
              "      <td>58.5</td>\n",
              "      <td>62.500000</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>children_playing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100263-2-0-121.wav</td>\n",
              "      <td>100263</td>\n",
              "      <td>60.5</td>\n",
              "      <td>64.500000</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>children_playing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100263-2-0-126.wav</td>\n",
              "      <td>100263</td>\n",
              "      <td>63.0</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>children_playing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100263-2-0-137.wav</td>\n",
              "      <td>100263</td>\n",
              "      <td>68.5</td>\n",
              "      <td>72.500000</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>children_playing</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f213291a-1e74-49a9-b553-324df3ff33c6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f213291a-1e74-49a9-b553-324df3ff33c6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f213291a-1e74-49a9-b553-324df3ff33c6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Balancing/Imbalancing Check\n",
        "metadata['class'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPstrhy5dR2P",
        "outputId": "2c3208be-c39a-4130-9d45-1e55ff962e72"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dog_bark            1000\n",
              "children_playing    1000\n",
              "air_conditioner     1000\n",
              "street_music        1000\n",
              "engine_idling       1000\n",
              "jackhammer          1000\n",
              "drilling            1000\n",
              "siren                929\n",
              "car_horn             429\n",
              "gun_shot             374\n",
              "Name: class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting MFCC's For every audio file\n",
        "def features_extractor(file_name):\n",
        "    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
        "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
        "    \n",
        "    return mfccs_scaled_features"
      ],
      "metadata": {
        "id": "xpQ46Y5aBxoK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Features extraction from all audio files (MFCC)\n",
        "extracted_features=[]\n",
        "for index_num,row in tqdm(metadata.iterrows()):\n",
        "    file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
        "    final_class_labels=row[\"class\"]\n",
        "    data=features_extractor(file_name)\n",
        "    extracted_features.append([data,final_class_labels])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK-JTFCCBz0a",
        "outputId": "502c3d1e-2aa1-4724-df98-a5a17d2a873b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3555it [55:06,  1.13it/s]/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=2048 is too small for input signal of length=1323\n",
            "  n_fft, y.shape[-1]\n",
            "8326it [2:06:42,  1.50it/s]/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=2048 is too small for input signal of length=1103\n",
            "  n_fft, y.shape[-1]\n",
            "8329it [2:06:43,  1.62it/s]/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=2048 is too small for input signal of length=1523\n",
            "  n_fft, y.shape[-1]\n",
            "8732it [2:12:23,  1.10it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting extracted_features to Pandas dataframe\n",
        "extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])\n",
        "extracted_features_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "GKktDnY_B11C",
        "outputId": "0c8f7cff-d691-44fd-8f37-07d5844d4051"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             feature             class\n",
              "0  [-214.95764, 70.502464, -130.70279, -53.116936...          dog_bark\n",
              "1  [-423.7311, 109.2299, -52.872597, 60.827538, 0...  children_playing\n",
              "2  [-458.52844, 121.35432, -46.535675, 51.969467,...  children_playing\n",
              "3  [-413.63254, 101.61351, -35.43868, 53.047146, ...  children_playing\n",
              "4  [-446.38693, 113.68634, -52.4572, 60.349724, 2...  children_playing"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7f006caa-60e2-4579-baf0-1d01292ab21c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[-214.95764, 70.502464, -130.70279, -53.116936...</td>\n",
              "      <td>dog_bark</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[-423.7311, 109.2299, -52.872597, 60.827538, 0...</td>\n",
              "      <td>children_playing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[-458.52844, 121.35432, -46.535675, 51.969467,...</td>\n",
              "      <td>children_playing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[-413.63254, 101.61351, -35.43868, 53.047146, ...</td>\n",
              "      <td>children_playing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[-446.38693, 113.68634, -52.4572, 60.349724, 2...</td>\n",
              "      <td>children_playing</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7f006caa-60e2-4579-baf0-1d01292ab21c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7f006caa-60e2-4579-baf0-1d01292ab21c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7f006caa-60e2-4579-baf0-1d01292ab21c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Frame Saving\n",
        "extracted_features_df.to_csv(\"UrbanSound8K_DF.csv\")"
      ],
      "metadata": {
        "id": "dQYkv2m4D-BW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Splitting\n",
        "X=np.array(extracted_features_df['feature'].tolist())\n",
        "y=np.array(extracted_features_df['class'].tolist())"
      ],
      "metadata": {
        "id": "EQ3voyKrEBq4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KpfVO52ECNF",
        "outputId": "924762ca-f60c-45ec-cdfe-3648f1681853"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8732, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MYC4X_-EGa2",
        "outputId": "d2d5e5f2-7e54-43df-cb30-788bd54088ee"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['dog_bark', 'children_playing', 'children_playing', ...,\n",
              "       'car_horn', 'car_horn', 'car_horn'], dtype='<U16')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Encoding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder=LabelEncoder()\n",
        "y=to_categorical(labelencoder.fit_transform(y))"
      ],
      "metadata": {
        "id": "vCEhcIGhEHSV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0rNciXCEJsQ",
        "outputId": "589d62e4-d15c-494c-8274-70d423318352"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Testing Sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
      ],
      "metadata": {
        "id": "w3vypd_EELRY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob43lJs4ENAa",
        "outputId": "253306ed-3ab7-417e-c540-675bc3984702"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.3082390e+02,  1.1256225e+02, -2.2600878e+01, ...,\n",
              "         3.2528090e+00, -1.3692100e+00,  2.7386472e+00],\n",
              "       [-1.3446434e+01,  9.1048195e+01, -7.8661475e+00, ...,\n",
              "        -3.2665925e+00, -5.2884049e+00, -1.5532947e+00],\n",
              "       [-4.9575279e+01,  2.3543632e-01, -2.0504959e+01, ...,\n",
              "         2.8775635e+00, -1.5828822e+00,  3.5109408e+00],\n",
              "       ...,\n",
              "       [-4.2677444e+02,  9.2583252e+01,  3.3174915e+00, ...,\n",
              "         7.9330575e-01,  7.1890563e-01,  7.1397936e-01],\n",
              "       [-1.4541722e+02,  1.3619025e+02, -3.3450352e+01, ...,\n",
              "         1.4591718e+00, -1.9928970e+00, -8.9318532e-01],\n",
              "       [-4.2099823e+02,  2.1074756e+02,  3.5814040e+00, ...,\n",
              "        -5.4054899e+00, -3.3959770e+00, -1.5590971e+00]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gacXyD5EO3E",
        "outputId": "d02c33d8-1d68-4ac4-b35d-59c44b713bcc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6985, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0rA8fbQERKY",
        "outputId": "e98eb50f-90d2-4a41-85bb-efd5399de533"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1747, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRLpSq1FES1B",
        "outputId": "ffd3ed77-fbc6-4ab2-d94f-00187be76173"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6985, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAdF7R90EUrc",
        "outputId": "21e28931-16e3-4300-9a81-a35bb1d44abb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1747, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mVLzfR1EWmK",
        "outputId": "0d42d186-b4ab-4f91-c6d0-6d9dcea7857e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout,Activation\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "ciiQmZBcEYkS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# No of classes\n",
        "num_labels=y.shape[1]\n",
        "print(num_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0B9gJjgJEaZ-",
        "outputId": "4cd2bb9c-664c-484b-dfaa-78bba8191b79"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "#first layer\n",
        "model.add(Dense(1600,input_shape=(40,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "#second layer\n",
        "model.add(Dense(800))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "#third layer\n",
        "model.add(Dense(400))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "#final layer\n",
        "model.add(Dense(num_labels))\n",
        "model.add(Activation('softmax'))"
      ],
      "metadata": {
        "id": "th3eo_KuEb-d"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQBssDzEEcEk",
        "outputId": "6f0138ea-1552-4b99-dc21-7df5336a2283"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 1600)              65600     \n",
            "                                                                 \n",
            " activation (Activation)     (None, 1600)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 800)               1280800   \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 800)               0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 800)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 400)               320400    \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 400)               0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 400)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                4010      \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,670,810\n",
            "Trainable params: 1,670,810\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')"
      ],
      "metadata": {
        "id": "rRoT4YJeEf1u"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from datetime import datetime "
      ],
      "metadata": {
        "id": "VqV5GcmqEiC9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 500\n",
        "num_batch_size = 128\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification.h5', verbose=1, save_best_only=True)\n",
        "start = datetime.now()\n",
        "\n",
        "results = model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "\n",
        "duration = datetime.now() - start\n",
        "print(\"Training completed in time: \", duration)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyrEFeIxEjgz",
        "outputId": "9ced5529-bbd1-4c4b-da8a-99fe1a8af49b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1129 - accuracy: 0.9711\n",
            "Epoch 1: val_loss improved from inf to 0.26749, saving model to saved_models/audio_classification.h5\n",
            "55/55 [==============================] - 1s 11ms/step - loss: 0.1129 - accuracy: 0.9711 - val_loss: 0.2675 - val_accuracy: 0.9439\n",
            "Epoch 2/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1100 - accuracy: 0.9707\n",
            "Epoch 2: val_loss improved from 0.26749 to 0.26522, saving model to saved_models/audio_classification.h5\n",
            "55/55 [==============================] - 1s 14ms/step - loss: 0.1099 - accuracy: 0.9705 - val_loss: 0.2652 - val_accuracy: 0.9468\n",
            "Epoch 3/500\n",
            "43/55 [======================>.......] - ETA: 0s - loss: 0.1273 - accuracy: 0.9695\n",
            "Epoch 3: val_loss did not improve from 0.26522\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1166 - accuracy: 0.9704 - val_loss: 0.2784 - val_accuracy: 0.9450\n",
            "Epoch 4/500\n",
            "44/55 [=======================>......] - ETA: 0s - loss: 0.1302 - accuracy: 0.9696\n",
            "Epoch 4: val_loss did not improve from 0.26522\n",
            "55/55 [==============================] - 0s 4ms/step - loss: 0.1238 - accuracy: 0.9704 - val_loss: 0.2760 - val_accuracy: 0.9433\n",
            "Epoch 5/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.1357 - accuracy: 0.9645\n",
            "Epoch 5: val_loss did not improve from 0.26522\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1344 - accuracy: 0.9655 - val_loss: 0.2957 - val_accuracy: 0.9382\n",
            "Epoch 6/500\n",
            "44/55 [=======================>......] - ETA: 0s - loss: 0.1227 - accuracy: 0.9689\n",
            "Epoch 6: val_loss did not improve from 0.26522\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1180 - accuracy: 0.9686 - val_loss: 0.2786 - val_accuracy: 0.9376\n",
            "Epoch 7/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.1214 - accuracy: 0.9680\n",
            "Epoch 7: val_loss did not improve from 0.26522\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1257 - accuracy: 0.9659 - val_loss: 0.2822 - val_accuracy: 0.9422\n",
            "Epoch 8/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1149 - accuracy: 0.9698\n",
            "Epoch 8: val_loss did not improve from 0.26522\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1149 - accuracy: 0.9698 - val_loss: 0.2813 - val_accuracy: 0.9410\n",
            "Epoch 9/500\n",
            "43/55 [======================>.......] - ETA: 0s - loss: 0.1375 - accuracy: 0.9660\n",
            "Epoch 9: val_loss did not improve from 0.26522\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1407 - accuracy: 0.9658 - val_loss: 0.2981 - val_accuracy: 0.9405\n",
            "Epoch 10/500\n",
            "44/55 [=======================>......] - ETA: 0s - loss: 0.1294 - accuracy: 0.9670\n",
            "Epoch 10: val_loss did not improve from 0.26522\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1286 - accuracy: 0.9672 - val_loss: 0.3023 - val_accuracy: 0.9393\n",
            "Epoch 11/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.1397 - accuracy: 0.9648\n",
            "Epoch 11: val_loss did not improve from 0.26522\n",
            "55/55 [==============================] - 0s 4ms/step - loss: 0.1440 - accuracy: 0.9654 - val_loss: 0.2889 - val_accuracy: 0.9422\n",
            "Epoch 12/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1059 - accuracy: 0.9706\n",
            "Epoch 12: val_loss did not improve from 0.26522\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1060 - accuracy: 0.9708 - val_loss: 0.3021 - val_accuracy: 0.9388\n",
            "Epoch 13/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.1236 - accuracy: 0.9689\n",
            "Epoch 13: val_loss did not improve from 0.26522\n",
            "55/55 [==============================] - 0s 4ms/step - loss: 0.1233 - accuracy: 0.9675 - val_loss: 0.2919 - val_accuracy: 0.9410\n",
            "Epoch 14/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.1169 - accuracy: 0.9701\n",
            "Epoch 14: val_loss did not improve from 0.26522\n",
            "55/55 [==============================] - 0s 4ms/step - loss: 0.1140 - accuracy: 0.9708 - val_loss: 0.2851 - val_accuracy: 0.9405\n",
            "Epoch 15/500\n",
            "44/55 [=======================>......] - ETA: 0s - loss: 0.1169 - accuracy: 0.9712\n",
            "Epoch 15: val_loss improved from 0.26522 to 0.25936, saving model to saved_models/audio_classification.h5\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1186 - accuracy: 0.9694 - val_loss: 0.2594 - val_accuracy: 0.9450\n",
            "Epoch 16/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.1058 - accuracy: 0.9692\n",
            "Epoch 16: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1050 - accuracy: 0.9694 - val_loss: 0.2727 - val_accuracy: 0.9468\n",
            "Epoch 17/500\n",
            "43/55 [======================>.......] - ETA: 0s - loss: 0.1137 - accuracy: 0.9720\n",
            "Epoch 17: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1124 - accuracy: 0.9728 - val_loss: 0.2899 - val_accuracy: 0.9513\n",
            "Epoch 18/500\n",
            "44/55 [=======================>......] - ETA: 0s - loss: 0.1053 - accuracy: 0.9751\n",
            "Epoch 18: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0977 - accuracy: 0.9764 - val_loss: 0.2959 - val_accuracy: 0.9450\n",
            "Epoch 19/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.1096 - accuracy: 0.9708\n",
            "Epoch 19: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1096 - accuracy: 0.9707 - val_loss: 0.2855 - val_accuracy: 0.9405\n",
            "Epoch 20/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.1056 - accuracy: 0.9706\n",
            "Epoch 20: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1086 - accuracy: 0.9702 - val_loss: 0.2786 - val_accuracy: 0.9422\n",
            "Epoch 21/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.0900 - accuracy: 0.9717\n",
            "Epoch 21: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 4ms/step - loss: 0.0969 - accuracy: 0.9709 - val_loss: 0.2844 - val_accuracy: 0.9468\n",
            "Epoch 22/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.0983 - accuracy: 0.9731\n",
            "Epoch 22: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 4ms/step - loss: 0.1008 - accuracy: 0.9735 - val_loss: 0.2692 - val_accuracy: 0.9473\n",
            "Epoch 23/500\n",
            "44/55 [=======================>......] - ETA: 0s - loss: 0.1060 - accuracy: 0.9737\n",
            "Epoch 23: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1127 - accuracy: 0.9737 - val_loss: 0.2794 - val_accuracy: 0.9445\n",
            "Epoch 24/500\n",
            "44/55 [=======================>......] - ETA: 0s - loss: 0.1043 - accuracy: 0.9711\n",
            "Epoch 24: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 4ms/step - loss: 0.1050 - accuracy: 0.9715 - val_loss: 0.2807 - val_accuracy: 0.9479\n",
            "Epoch 25/500\n",
            "44/55 [=======================>......] - ETA: 0s - loss: 0.1194 - accuracy: 0.9680\n",
            "Epoch 25: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 4ms/step - loss: 0.1154 - accuracy: 0.9705 - val_loss: 0.2710 - val_accuracy: 0.9485\n",
            "Epoch 26/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.1079 - accuracy: 0.9704\n",
            "Epoch 26: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1023 - accuracy: 0.9715 - val_loss: 0.2947 - val_accuracy: 0.9422\n",
            "Epoch 27/500\n",
            "44/55 [=======================>......] - ETA: 0s - loss: 0.1146 - accuracy: 0.9696\n",
            "Epoch 27: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1122 - accuracy: 0.9711 - val_loss: 0.3014 - val_accuracy: 0.9405\n",
            "Epoch 28/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.0969 - accuracy: 0.9671\n",
            "Epoch 28: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.0963 - accuracy: 0.9672 - val_loss: 0.2913 - val_accuracy: 0.9422\n",
            "Epoch 29/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1215 - accuracy: 0.9698\n",
            "Epoch 29: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.1233 - accuracy: 0.9698 - val_loss: 0.2781 - val_accuracy: 0.9422\n",
            "Epoch 30/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.1409 - accuracy: 0.9663\n",
            "Epoch 30: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1392 - accuracy: 0.9649 - val_loss: 0.3283 - val_accuracy: 0.9347\n",
            "Epoch 31/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.1234 - accuracy: 0.9714\n",
            "Epoch 31: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1287 - accuracy: 0.9692 - val_loss: 0.2841 - val_accuracy: 0.9416\n",
            "Epoch 32/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.1243 - accuracy: 0.9648\n",
            "Epoch 32: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1227 - accuracy: 0.9654 - val_loss: 0.2798 - val_accuracy: 0.9422\n",
            "Epoch 33/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.1032 - accuracy: 0.9714\n",
            "Epoch 33: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1052 - accuracy: 0.9708 - val_loss: 0.2947 - val_accuracy: 0.9319\n",
            "Epoch 34/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.1173 - accuracy: 0.9700\n",
            "Epoch 34: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1195 - accuracy: 0.9681 - val_loss: 0.2711 - val_accuracy: 0.9370\n",
            "Epoch 35/500\n",
            "43/55 [======================>.......] - ETA: 0s - loss: 0.1257 - accuracy: 0.9680\n",
            "Epoch 35: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1288 - accuracy: 0.9674 - val_loss: 0.2679 - val_accuracy: 0.9445\n",
            "Epoch 36/500\n",
            "44/55 [=======================>......] - ETA: 0s - loss: 0.1326 - accuracy: 0.9664\n",
            "Epoch 36: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1255 - accuracy: 0.9682 - val_loss: 0.2657 - val_accuracy: 0.9433\n",
            "Epoch 37/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1252 - accuracy: 0.9689\n",
            "Epoch 37: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1252 - accuracy: 0.9689 - val_loss: 0.2642 - val_accuracy: 0.9439\n",
            "Epoch 38/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.1366 - accuracy: 0.9676\n",
            "Epoch 38: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1357 - accuracy: 0.9679 - val_loss: 0.2992 - val_accuracy: 0.9353\n",
            "Epoch 39/500\n",
            "44/55 [=======================>......] - ETA: 0s - loss: 0.1111 - accuracy: 0.9695\n",
            "Epoch 39: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1196 - accuracy: 0.9695 - val_loss: 0.2758 - val_accuracy: 0.9456\n",
            "Epoch 40/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9698\n",
            "Epoch 40: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1235 - accuracy: 0.9701 - val_loss: 0.3281 - val_accuracy: 0.9416\n",
            "Epoch 41/500\n",
            "43/55 [======================>.......] - ETA: 0s - loss: 0.1080 - accuracy: 0.9713\n",
            "Epoch 41: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1062 - accuracy: 0.9719 - val_loss: 0.3307 - val_accuracy: 0.9445\n",
            "Epoch 42/500\n",
            "43/55 [======================>.......] - ETA: 0s - loss: 0.1160 - accuracy: 0.9698\n",
            "Epoch 42: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1149 - accuracy: 0.9701 - val_loss: 0.3254 - val_accuracy: 0.9365\n",
            "Epoch 43/500\n",
            "44/55 [=======================>......] - ETA: 0s - loss: 0.1176 - accuracy: 0.9686\n",
            "Epoch 43: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1187 - accuracy: 0.9686 - val_loss: 0.3286 - val_accuracy: 0.9353\n",
            "Epoch 44/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1043 - accuracy: 0.9702\n",
            "Epoch 44: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1043 - accuracy: 0.9702 - val_loss: 0.3247 - val_accuracy: 0.9428\n",
            "Epoch 45/500\n",
            "44/55 [=======================>......] - ETA: 0s - loss: 0.1012 - accuracy: 0.9739\n",
            "Epoch 45: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1065 - accuracy: 0.9718 - val_loss: 0.3146 - val_accuracy: 0.9410\n",
            "Epoch 46/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.1293 - accuracy: 0.9695\n",
            "Epoch 46: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1245 - accuracy: 0.9702 - val_loss: 0.3017 - val_accuracy: 0.9416\n",
            "Epoch 47/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1091 - accuracy: 0.9721\n",
            "Epoch 47: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1091 - accuracy: 0.9721 - val_loss: 0.2790 - val_accuracy: 0.9433\n",
            "Epoch 48/500\n",
            "44/55 [=======================>......] - ETA: 0s - loss: 0.1108 - accuracy: 0.9728\n",
            "Epoch 48: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1104 - accuracy: 0.9722 - val_loss: 0.2666 - val_accuracy: 0.9502\n",
            "Epoch 49/500\n",
            "43/55 [======================>.......] - ETA: 0s - loss: 0.1193 - accuracy: 0.9691\n",
            "Epoch 49: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1360 - accuracy: 0.9685 - val_loss: 0.3227 - val_accuracy: 0.9416\n",
            "Epoch 50/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.1331 - accuracy: 0.9642\n",
            "Epoch 50: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1350 - accuracy: 0.9641 - val_loss: 0.3283 - val_accuracy: 0.9479\n",
            "Epoch 51/500\n",
            "43/55 [======================>.......] - ETA: 0s - loss: 0.1170 - accuracy: 0.9709\n",
            "Epoch 51: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1295 - accuracy: 0.9689 - val_loss: 0.3180 - val_accuracy: 0.9450\n",
            "Epoch 52/500\n",
            "44/55 [=======================>......] - ETA: 0s - loss: 0.1190 - accuracy: 0.9677\n",
            "Epoch 52: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1129 - accuracy: 0.9689 - val_loss: 0.2946 - val_accuracy: 0.9433\n",
            "Epoch 53/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.0959 - accuracy: 0.9738\n",
            "Epoch 53: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1002 - accuracy: 0.9731 - val_loss: 0.2997 - val_accuracy: 0.9462\n",
            "Epoch 54/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.1223 - accuracy: 0.9704\n",
            "Epoch 54: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1364 - accuracy: 0.9675 - val_loss: 0.2977 - val_accuracy: 0.9462\n",
            "Epoch 55/500\n",
            "41/55 [=====================>........] - ETA: 0s - loss: 0.1480 - accuracy: 0.9678\n",
            "Epoch 55: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1407 - accuracy: 0.9674 - val_loss: 0.2806 - val_accuracy: 0.9456\n",
            "Epoch 56/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.1091 - accuracy: 0.9678\n",
            "Epoch 56: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1021 - accuracy: 0.9702 - val_loss: 0.2919 - val_accuracy: 0.9422\n",
            "Epoch 57/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.1013 - accuracy: 0.9736\n",
            "Epoch 57: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0957 - accuracy: 0.9749 - val_loss: 0.2812 - val_accuracy: 0.9462\n",
            "Epoch 58/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.1031 - accuracy: 0.9729\n",
            "Epoch 58: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1041 - accuracy: 0.9729 - val_loss: 0.3088 - val_accuracy: 0.9422\n",
            "Epoch 59/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.1270 - accuracy: 0.9649\n",
            "Epoch 59: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 4ms/step - loss: 0.1247 - accuracy: 0.9658 - val_loss: 0.3013 - val_accuracy: 0.9445\n",
            "Epoch 60/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1292 - accuracy: 0.9671\n",
            "Epoch 60: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1292 - accuracy: 0.9671 - val_loss: 0.2843 - val_accuracy: 0.9439\n",
            "Epoch 61/500\n",
            "43/55 [======================>.......] - ETA: 0s - loss: 0.1124 - accuracy: 0.9713\n",
            "Epoch 61: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1121 - accuracy: 0.9715 - val_loss: 0.2785 - val_accuracy: 0.9445\n",
            "Epoch 62/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1293 - accuracy: 0.9671\n",
            "Epoch 62: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1293 - accuracy: 0.9671 - val_loss: 0.2886 - val_accuracy: 0.9462\n",
            "Epoch 63/500\n",
            "43/55 [======================>.......] - ETA: 0s - loss: 0.1242 - accuracy: 0.9653\n",
            "Epoch 63: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1214 - accuracy: 0.9666 - val_loss: 0.2981 - val_accuracy: 0.9439\n",
            "Epoch 64/500\n",
            "43/55 [======================>.......] - ETA: 0s - loss: 0.0896 - accuracy: 0.9767\n",
            "Epoch 64: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0994 - accuracy: 0.9745 - val_loss: 0.3000 - val_accuracy: 0.9393\n",
            "Epoch 65/500\n",
            "43/55 [======================>.......] - ETA: 0s - loss: 0.0992 - accuracy: 0.9731\n",
            "Epoch 65: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0956 - accuracy: 0.9738 - val_loss: 0.2946 - val_accuracy: 0.9439\n",
            "Epoch 66/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.1329 - accuracy: 0.9679\n",
            "Epoch 66: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1324 - accuracy: 0.9679 - val_loss: 0.2850 - val_accuracy: 0.9485\n",
            "Epoch 67/500\n",
            "44/55 [=======================>......] - ETA: 0s - loss: 0.1442 - accuracy: 0.9686\n",
            "Epoch 67: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1362 - accuracy: 0.9691 - val_loss: 0.3162 - val_accuracy: 0.9445\n",
            "Epoch 68/500\n",
            "44/55 [=======================>......] - ETA: 0s - loss: 0.1261 - accuracy: 0.9661\n",
            "Epoch 68: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1257 - accuracy: 0.9676 - val_loss: 0.2705 - val_accuracy: 0.9433\n",
            "Epoch 69/500\n",
            "44/55 [=======================>......] - ETA: 0s - loss: 0.1123 - accuracy: 0.9723\n",
            "Epoch 69: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1155 - accuracy: 0.9722 - val_loss: 0.2786 - val_accuracy: 0.9388\n",
            "Epoch 70/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1081 - accuracy: 0.9732\n",
            "Epoch 70: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1081 - accuracy: 0.9732 - val_loss: 0.2850 - val_accuracy: 0.9479\n",
            "Epoch 71/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.1118 - accuracy: 0.9699\n",
            "Epoch 71: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1121 - accuracy: 0.9701 - val_loss: 0.2878 - val_accuracy: 0.9399\n",
            "Epoch 72/500\n",
            "44/55 [=======================>......] - ETA: 0s - loss: 0.1340 - accuracy: 0.9668\n",
            "Epoch 72: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1369 - accuracy: 0.9648 - val_loss: 0.2870 - val_accuracy: 0.9428\n",
            "Epoch 73/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1417 - accuracy: 0.9671\n",
            "Epoch 73: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1417 - accuracy: 0.9671 - val_loss: 0.2845 - val_accuracy: 0.9382\n",
            "Epoch 74/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.1214 - accuracy: 0.9654\n",
            "Epoch 74: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1218 - accuracy: 0.9656 - val_loss: 0.2773 - val_accuracy: 0.9410\n",
            "Epoch 75/500\n",
            "43/55 [======================>.......] - ETA: 0s - loss: 0.0989 - accuracy: 0.9749\n",
            "Epoch 75: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 4ms/step - loss: 0.0971 - accuracy: 0.9738 - val_loss: 0.2755 - val_accuracy: 0.9370\n",
            "Epoch 76/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.0947 - accuracy: 0.9732\n",
            "Epoch 76: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0901 - accuracy: 0.9737 - val_loss: 0.2893 - val_accuracy: 0.9450\n",
            "Epoch 77/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.0900 - accuracy: 0.9750\n",
            "Epoch 77: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0899 - accuracy: 0.9748 - val_loss: 0.2777 - val_accuracy: 0.9462\n",
            "Epoch 78/500\n",
            "43/55 [======================>.......] - ETA: 0s - loss: 0.0810 - accuracy: 0.9769\n",
            "Epoch 78: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0767 - accuracy: 0.9772 - val_loss: 0.2820 - val_accuracy: 0.9422\n",
            "Epoch 79/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.0894 - accuracy: 0.9762\n",
            "Epoch 79: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0949 - accuracy: 0.9772 - val_loss: 0.3037 - val_accuracy: 0.9388\n",
            "Epoch 80/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1141 - accuracy: 0.9707\n",
            "Epoch 80: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1141 - accuracy: 0.9707 - val_loss: 0.2891 - val_accuracy: 0.9388\n",
            "Epoch 81/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1232 - accuracy: 0.9678\n",
            "Epoch 81: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1232 - accuracy: 0.9678 - val_loss: 0.3114 - val_accuracy: 0.9393\n",
            "Epoch 82/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.1188 - accuracy: 0.9669\n",
            "Epoch 82: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1165 - accuracy: 0.9678 - val_loss: 0.3181 - val_accuracy: 0.9359\n",
            "Epoch 83/500\n",
            "43/55 [======================>.......] - ETA: 0s - loss: 0.1102 - accuracy: 0.9709\n",
            "Epoch 83: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1110 - accuracy: 0.9711 - val_loss: 0.2806 - val_accuracy: 0.9473\n",
            "Epoch 84/500\n",
            "43/55 [======================>.......] - ETA: 0s - loss: 0.1085 - accuracy: 0.9711\n",
            "Epoch 84: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1098 - accuracy: 0.9709 - val_loss: 0.3033 - val_accuracy: 0.9445\n",
            "Epoch 85/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.1215 - accuracy: 0.9658\n",
            "Epoch 85: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1187 - accuracy: 0.9679 - val_loss: 0.3372 - val_accuracy: 0.9428\n",
            "Epoch 86/500\n",
            "43/55 [======================>.......] - ETA: 0s - loss: 0.1144 - accuracy: 0.9704\n",
            "Epoch 86: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1168 - accuracy: 0.9685 - val_loss: 0.3119 - val_accuracy: 0.9485\n",
            "Epoch 87/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.0887 - accuracy: 0.9729\n",
            "Epoch 87: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0855 - accuracy: 0.9739 - val_loss: 0.2650 - val_accuracy: 0.9491\n",
            "Epoch 88/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.1349 - accuracy: 0.9673\n",
            "Epoch 88: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1339 - accuracy: 0.9675 - val_loss: 0.3378 - val_accuracy: 0.9416\n",
            "Epoch 89/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1319 - accuracy: 0.9694\n",
            "Epoch 89: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1319 - accuracy: 0.9694 - val_loss: 0.2980 - val_accuracy: 0.9445\n",
            "Epoch 90/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.1037 - accuracy: 0.9727\n",
            "Epoch 90: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1027 - accuracy: 0.9729 - val_loss: 0.2751 - val_accuracy: 0.9468\n",
            "Epoch 91/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.1143 - accuracy: 0.9726\n",
            "Epoch 91: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1132 - accuracy: 0.9725 - val_loss: 0.2612 - val_accuracy: 0.9462\n",
            "Epoch 92/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.0955 - accuracy: 0.9737\n",
            "Epoch 92: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0955 - accuracy: 0.9737 - val_loss: 0.2849 - val_accuracy: 0.9410\n",
            "Epoch 93/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.1346 - accuracy: 0.9688\n",
            "Epoch 93: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1246 - accuracy: 0.9707 - val_loss: 0.3138 - val_accuracy: 0.9416\n",
            "Epoch 94/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.1327 - accuracy: 0.9669\n",
            "Epoch 94: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1319 - accuracy: 0.9658 - val_loss: 0.2767 - val_accuracy: 0.9491\n",
            "Epoch 95/500\n",
            "43/55 [======================>.......] - ETA: 0s - loss: 0.1210 - accuracy: 0.9698\n",
            "Epoch 95: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1186 - accuracy: 0.9704 - val_loss: 0.3026 - val_accuracy: 0.9508\n",
            "Epoch 96/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.1185 - accuracy: 0.9723\n",
            "Epoch 96: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1115 - accuracy: 0.9731 - val_loss: 0.3041 - val_accuracy: 0.9416\n",
            "Epoch 97/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.1291 - accuracy: 0.9679\n",
            "Epoch 97: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1296 - accuracy: 0.9675 - val_loss: 0.2925 - val_accuracy: 0.9405\n",
            "Epoch 98/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1297 - accuracy: 0.9668\n",
            "Epoch 98: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1297 - accuracy: 0.9668 - val_loss: 0.2837 - val_accuracy: 0.9468\n",
            "Epoch 99/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.1051 - accuracy: 0.9684\n",
            "Epoch 99: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1098 - accuracy: 0.9688 - val_loss: 0.3022 - val_accuracy: 0.9456\n",
            "Epoch 100/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1048 - accuracy: 0.9751\n",
            "Epoch 100: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1048 - accuracy: 0.9751 - val_loss: 0.3246 - val_accuracy: 0.9393\n",
            "Epoch 101/500\n",
            "43/55 [======================>.......] - ETA: 0s - loss: 0.1504 - accuracy: 0.9660\n",
            "Epoch 101: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1470 - accuracy: 0.9671 - val_loss: 0.3160 - val_accuracy: 0.9422\n",
            "Epoch 102/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.1075 - accuracy: 0.9701\n",
            "Epoch 102: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1077 - accuracy: 0.9698 - val_loss: 0.3065 - val_accuracy: 0.9450\n",
            "Epoch 103/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1351 - accuracy: 0.9691\n",
            "Epoch 103: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1351 - accuracy: 0.9691 - val_loss: 0.2907 - val_accuracy: 0.9468\n",
            "Epoch 104/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.1251 - accuracy: 0.9725\n",
            "Epoch 104: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1197 - accuracy: 0.9719 - val_loss: 0.2982 - val_accuracy: 0.9468\n",
            "Epoch 105/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.1157 - accuracy: 0.9711\n",
            "Epoch 105: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1153 - accuracy: 0.9709 - val_loss: 0.2950 - val_accuracy: 0.9439\n",
            "Epoch 106/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.1009 - accuracy: 0.9736\n",
            "Epoch 106: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0946 - accuracy: 0.9748 - val_loss: 0.3120 - val_accuracy: 0.9462\n",
            "Epoch 107/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.1140 - accuracy: 0.9743\n",
            "Epoch 107: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1187 - accuracy: 0.9722 - val_loss: 0.3092 - val_accuracy: 0.9462\n",
            "Epoch 108/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1141 - accuracy: 0.9695\n",
            "Epoch 108: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1134 - accuracy: 0.9701 - val_loss: 0.2919 - val_accuracy: 0.9508\n",
            "Epoch 109/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.0968 - accuracy: 0.9755\n",
            "Epoch 109: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0958 - accuracy: 0.9755 - val_loss: 0.2784 - val_accuracy: 0.9479\n",
            "Epoch 110/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1135 - accuracy: 0.9699\n",
            "Epoch 110: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1135 - accuracy: 0.9699 - val_loss: 0.3092 - val_accuracy: 0.9496\n",
            "Epoch 111/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.1363 - accuracy: 0.9650\n",
            "Epoch 111: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1360 - accuracy: 0.9651 - val_loss: 0.2842 - val_accuracy: 0.9445\n",
            "Epoch 112/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.0936 - accuracy: 0.9758\n",
            "Epoch 112: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0936 - accuracy: 0.9757 - val_loss: 0.2597 - val_accuracy: 0.9439\n",
            "Epoch 113/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.0960 - accuracy: 0.9751\n",
            "Epoch 113: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0953 - accuracy: 0.9752 - val_loss: 0.2742 - val_accuracy: 0.9485\n",
            "Epoch 114/500\n",
            "43/55 [======================>.......] - ETA: 0s - loss: 0.1030 - accuracy: 0.9724\n",
            "Epoch 114: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1041 - accuracy: 0.9727 - val_loss: 0.2786 - val_accuracy: 0.9462\n",
            "Epoch 115/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1132 - accuracy: 0.9691\n",
            "Epoch 115: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1132 - accuracy: 0.9691 - val_loss: 0.3216 - val_accuracy: 0.9416\n",
            "Epoch 116/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9716\n",
            "Epoch 116: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1181 - accuracy: 0.9719 - val_loss: 0.2893 - val_accuracy: 0.9399\n",
            "Epoch 117/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1428 - accuracy: 0.9679\n",
            "Epoch 117: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1428 - accuracy: 0.9679 - val_loss: 0.3090 - val_accuracy: 0.9428\n",
            "Epoch 118/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.1356 - accuracy: 0.9673\n",
            "Epoch 118: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1247 - accuracy: 0.9684 - val_loss: 0.2921 - val_accuracy: 0.9439\n",
            "Epoch 119/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.1051 - accuracy: 0.9698\n",
            "Epoch 119: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1070 - accuracy: 0.9699 - val_loss: 0.3059 - val_accuracy: 0.9456\n",
            "Epoch 120/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1205 - accuracy: 0.9686\n",
            "Epoch 120: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1205 - accuracy: 0.9686 - val_loss: 0.2979 - val_accuracy: 0.9473\n",
            "Epoch 121/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.1201 - accuracy: 0.9708\n",
            "Epoch 121: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1208 - accuracy: 0.9705 - val_loss: 0.2893 - val_accuracy: 0.9439\n",
            "Epoch 122/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.1047 - accuracy: 0.9719\n",
            "Epoch 122: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1039 - accuracy: 0.9721 - val_loss: 0.2899 - val_accuracy: 0.9450\n",
            "Epoch 123/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9712\n",
            "Epoch 123: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1210 - accuracy: 0.9711 - val_loss: 0.2714 - val_accuracy: 0.9462\n",
            "Epoch 124/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9708\n",
            "Epoch 124: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1014 - accuracy: 0.9708 - val_loss: 0.2915 - val_accuracy: 0.9508\n",
            "Epoch 125/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.0850 - accuracy: 0.9721\n",
            "Epoch 125: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0868 - accuracy: 0.9729 - val_loss: 0.3153 - val_accuracy: 0.9439\n",
            "Epoch 126/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.1019 - accuracy: 0.9746\n",
            "Epoch 126: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1028 - accuracy: 0.9744 - val_loss: 0.2893 - val_accuracy: 0.9468\n",
            "Epoch 127/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.1452 - accuracy: 0.9666\n",
            "Epoch 127: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1441 - accuracy: 0.9666 - val_loss: 0.2878 - val_accuracy: 0.9445\n",
            "Epoch 128/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.1349 - accuracy: 0.9714\n",
            "Epoch 128: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1369 - accuracy: 0.9707 - val_loss: 0.2977 - val_accuracy: 0.9422\n",
            "Epoch 129/500\n",
            "42/55 [=====================>........] - ETA: 0s - loss: 0.1312 - accuracy: 0.9680\n",
            "Epoch 129: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1276 - accuracy: 0.9675 - val_loss: 0.2832 - val_accuracy: 0.9445\n",
            "Epoch 130/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1060 - accuracy: 0.9724\n",
            "Epoch 130: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1120 - accuracy: 0.9721 - val_loss: 0.2853 - val_accuracy: 0.9388\n",
            "Epoch 131/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1444 - accuracy: 0.9701\n",
            "Epoch 131: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1444 - accuracy: 0.9701 - val_loss: 0.3075 - val_accuracy: 0.9468\n",
            "Epoch 132/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.1496 - accuracy: 0.9670\n",
            "Epoch 132: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1483 - accuracy: 0.9672 - val_loss: 0.2881 - val_accuracy: 0.9422\n",
            "Epoch 133/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.1172 - accuracy: 0.9689\n",
            "Epoch 133: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1175 - accuracy: 0.9685 - val_loss: 0.3014 - val_accuracy: 0.9405\n",
            "Epoch 134/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1023 - accuracy: 0.9703\n",
            "Epoch 134: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1014 - accuracy: 0.9704 - val_loss: 0.2982 - val_accuracy: 0.9382\n",
            "Epoch 135/500\n",
            "43/55 [======================>.......] - ETA: 0s - loss: 0.0915 - accuracy: 0.9738\n",
            "Epoch 135: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1054 - accuracy: 0.9724 - val_loss: 0.2847 - val_accuracy: 0.9422\n",
            "Epoch 136/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1122 - accuracy: 0.9721\n",
            "Epoch 136: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 1s 9ms/step - loss: 0.1129 - accuracy: 0.9721 - val_loss: 0.2987 - val_accuracy: 0.9450\n",
            "Epoch 137/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1156 - accuracy: 0.9745\n",
            "Epoch 137: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.1160 - accuracy: 0.9742 - val_loss: 0.3011 - val_accuracy: 0.9439\n",
            "Epoch 138/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.0990 - accuracy: 0.9744\n",
            "Epoch 138: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0990 - accuracy: 0.9744 - val_loss: 0.3361 - val_accuracy: 0.9393\n",
            "Epoch 139/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1096 - accuracy: 0.9722\n",
            "Epoch 139: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.1069 - accuracy: 0.9724 - val_loss: 0.3020 - val_accuracy: 0.9479\n",
            "Epoch 140/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.0999 - accuracy: 0.9732\n",
            "Epoch 140: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 1s 10ms/step - loss: 0.0994 - accuracy: 0.9734 - val_loss: 0.2951 - val_accuracy: 0.9456\n",
            "Epoch 141/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1282 - accuracy: 0.9708\n",
            "Epoch 141: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.1282 - accuracy: 0.9708 - val_loss: 0.3117 - val_accuracy: 0.9513\n",
            "Epoch 142/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1137 - accuracy: 0.9718\n",
            "Epoch 142: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1125 - accuracy: 0.9711 - val_loss: 0.3222 - val_accuracy: 0.9399\n",
            "Epoch 143/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.1107 - accuracy: 0.9727\n",
            "Epoch 143: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 1s 12ms/step - loss: 0.1092 - accuracy: 0.9731 - val_loss: 0.3162 - val_accuracy: 0.9422\n",
            "Epoch 144/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1117 - accuracy: 0.9711\n",
            "Epoch 144: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.1123 - accuracy: 0.9714 - val_loss: 0.3031 - val_accuracy: 0.9399\n",
            "Epoch 145/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.1073 - accuracy: 0.9706\n",
            "Epoch 145: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1069 - accuracy: 0.9707 - val_loss: 0.3391 - val_accuracy: 0.9359\n",
            "Epoch 146/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1524 - accuracy: 0.9700\n",
            "Epoch 146: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1486 - accuracy: 0.9704 - val_loss: 0.3201 - val_accuracy: 0.9433\n",
            "Epoch 147/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.1184 - accuracy: 0.9717\n",
            "Epoch 147: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1179 - accuracy: 0.9718 - val_loss: 0.3165 - val_accuracy: 0.9399\n",
            "Epoch 148/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.1225 - accuracy: 0.9708\n",
            "Epoch 148: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1288 - accuracy: 0.9702 - val_loss: 0.2817 - val_accuracy: 0.9445\n",
            "Epoch 149/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1123 - accuracy: 0.9697\n",
            "Epoch 149: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1127 - accuracy: 0.9699 - val_loss: 0.3136 - val_accuracy: 0.9445\n",
            "Epoch 150/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.1190 - accuracy: 0.9707\n",
            "Epoch 150: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1231 - accuracy: 0.9707 - val_loss: 0.3175 - val_accuracy: 0.9439\n",
            "Epoch 151/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.0946 - accuracy: 0.9746\n",
            "Epoch 151: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0952 - accuracy: 0.9747 - val_loss: 0.2889 - val_accuracy: 0.9508\n",
            "Epoch 152/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1281 - accuracy: 0.9736\n",
            "Epoch 152: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1322 - accuracy: 0.9742 - val_loss: 0.2851 - val_accuracy: 0.9496\n",
            "Epoch 153/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1287 - accuracy: 0.9730\n",
            "Epoch 153: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1278 - accuracy: 0.9732 - val_loss: 0.3073 - val_accuracy: 0.9445\n",
            "Epoch 154/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1149 - accuracy: 0.9710\n",
            "Epoch 154: val_loss did not improve from 0.25936\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1156 - accuracy: 0.9711 - val_loss: 0.3118 - val_accuracy: 0.9473\n",
            "Epoch 155/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1325 - accuracy: 0.9701\n",
            "Epoch 155: val_loss improved from 0.25936 to 0.25094, saving model to saved_models/audio_classification.h5\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1304 - accuracy: 0.9702 - val_loss: 0.2509 - val_accuracy: 0.9473\n",
            "Epoch 156/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1247 - accuracy: 0.9718\n",
            "Epoch 156: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1253 - accuracy: 0.9712 - val_loss: 0.3278 - val_accuracy: 0.9388\n",
            "Epoch 157/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.0994 - accuracy: 0.9706\n",
            "Epoch 157: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1026 - accuracy: 0.9705 - val_loss: 0.3385 - val_accuracy: 0.9393\n",
            "Epoch 158/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1225 - accuracy: 0.9707\n",
            "Epoch 158: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1246 - accuracy: 0.9707 - val_loss: 0.3870 - val_accuracy: 0.9359\n",
            "Epoch 159/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1350 - accuracy: 0.9686\n",
            "Epoch 159: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1374 - accuracy: 0.9686 - val_loss: 0.3601 - val_accuracy: 0.9422\n",
            "Epoch 160/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1055 - accuracy: 0.9702\n",
            "Epoch 160: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1045 - accuracy: 0.9701 - val_loss: 0.3136 - val_accuracy: 0.9433\n",
            "Epoch 161/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.0893 - accuracy: 0.9737\n",
            "Epoch 161: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0866 - accuracy: 0.9745 - val_loss: 0.3321 - val_accuracy: 0.9456\n",
            "Epoch 162/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.0986 - accuracy: 0.9755\n",
            "Epoch 162: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0983 - accuracy: 0.9744 - val_loss: 0.3372 - val_accuracy: 0.9428\n",
            "Epoch 163/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.0902 - accuracy: 0.9760\n",
            "Epoch 163: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0940 - accuracy: 0.9757 - val_loss: 0.3567 - val_accuracy: 0.9416\n",
            "Epoch 164/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1219 - accuracy: 0.9729\n",
            "Epoch 164: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1176 - accuracy: 0.9735 - val_loss: 0.3385 - val_accuracy: 0.9410\n",
            "Epoch 165/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1249 - accuracy: 0.9683\n",
            "Epoch 165: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1263 - accuracy: 0.9686 - val_loss: 0.3344 - val_accuracy: 0.9410\n",
            "Epoch 166/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1217 - accuracy: 0.9712\n",
            "Epoch 166: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1196 - accuracy: 0.9709 - val_loss: 0.3189 - val_accuracy: 0.9399\n",
            "Epoch 167/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1205 - accuracy: 0.9686\n",
            "Epoch 167: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1195 - accuracy: 0.9689 - val_loss: 0.3036 - val_accuracy: 0.9491\n",
            "Epoch 168/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1159 - accuracy: 0.9720\n",
            "Epoch 168: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1195 - accuracy: 0.9714 - val_loss: 0.3272 - val_accuracy: 0.9433\n",
            "Epoch 169/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1028 - accuracy: 0.9755\n",
            "Epoch 169: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1032 - accuracy: 0.9757 - val_loss: 0.3158 - val_accuracy: 0.9450\n",
            "Epoch 170/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.0867 - accuracy: 0.9761\n",
            "Epoch 170: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0855 - accuracy: 0.9762 - val_loss: 0.3036 - val_accuracy: 0.9485\n",
            "Epoch 171/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.0961 - accuracy: 0.9763\n",
            "Epoch 171: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0988 - accuracy: 0.9757 - val_loss: 0.3079 - val_accuracy: 0.9479\n",
            "Epoch 172/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.0992 - accuracy: 0.9748\n",
            "Epoch 172: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1019 - accuracy: 0.9745 - val_loss: 0.2949 - val_accuracy: 0.9473\n",
            "Epoch 173/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1095 - accuracy: 0.9746\n",
            "Epoch 173: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1105 - accuracy: 0.9745 - val_loss: 0.3004 - val_accuracy: 0.9422\n",
            "Epoch 174/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1333 - accuracy: 0.9752\n",
            "Epoch 174: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1334 - accuracy: 0.9748 - val_loss: 0.2838 - val_accuracy: 0.9439\n",
            "Epoch 175/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1053 - accuracy: 0.9714\n",
            "Epoch 175: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1059 - accuracy: 0.9718 - val_loss: 0.2628 - val_accuracy: 0.9433\n",
            "Epoch 176/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.0925 - accuracy: 0.9738\n",
            "Epoch 176: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0916 - accuracy: 0.9735 - val_loss: 0.2870 - val_accuracy: 0.9496\n",
            "Epoch 177/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.1279 - accuracy: 0.9732\n",
            "Epoch 177: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1292 - accuracy: 0.9728 - val_loss: 0.3308 - val_accuracy: 0.9388\n",
            "Epoch 178/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1188 - accuracy: 0.9700\n",
            "Epoch 178: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1159 - accuracy: 0.9705 - val_loss: 0.3178 - val_accuracy: 0.9416\n",
            "Epoch 179/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.0999 - accuracy: 0.9724\n",
            "Epoch 179: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1089 - accuracy: 0.9721 - val_loss: 0.2972 - val_accuracy: 0.9456\n",
            "Epoch 180/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1153 - accuracy: 0.9719\n",
            "Epoch 180: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1160 - accuracy: 0.9715 - val_loss: 0.2922 - val_accuracy: 0.9445\n",
            "Epoch 181/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1149 - accuracy: 0.9704\n",
            "Epoch 181: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1127 - accuracy: 0.9709 - val_loss: 0.2921 - val_accuracy: 0.9479\n",
            "Epoch 182/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1102 - accuracy: 0.9748\n",
            "Epoch 182: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1099 - accuracy: 0.9749 - val_loss: 0.3055 - val_accuracy: 0.9393\n",
            "Epoch 183/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1203 - accuracy: 0.9703\n",
            "Epoch 183: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1182 - accuracy: 0.9698 - val_loss: 0.3109 - val_accuracy: 0.9445\n",
            "Epoch 184/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1482 - accuracy: 0.9678\n",
            "Epoch 184: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1417 - accuracy: 0.9692 - val_loss: 0.3033 - val_accuracy: 0.9439\n",
            "Epoch 185/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1396 - accuracy: 0.9645\n",
            "Epoch 185: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1409 - accuracy: 0.9645 - val_loss: 0.3105 - val_accuracy: 0.9399\n",
            "Epoch 186/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1165 - accuracy: 0.9740\n",
            "Epoch 186: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1161 - accuracy: 0.9741 - val_loss: 0.3260 - val_accuracy: 0.9433\n",
            "Epoch 187/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1066 - accuracy: 0.9694\n",
            "Epoch 187: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1032 - accuracy: 0.9705 - val_loss: 0.3318 - val_accuracy: 0.9416\n",
            "Epoch 188/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1182 - accuracy: 0.9704\n",
            "Epoch 188: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1173 - accuracy: 0.9709 - val_loss: 0.3101 - val_accuracy: 0.9462\n",
            "Epoch 189/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1122 - accuracy: 0.9707\n",
            "Epoch 189: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1106 - accuracy: 0.9708 - val_loss: 0.2940 - val_accuracy: 0.9473\n",
            "Epoch 190/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.0961 - accuracy: 0.9746\n",
            "Epoch 190: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0975 - accuracy: 0.9737 - val_loss: 0.3074 - val_accuracy: 0.9456\n",
            "Epoch 191/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1235 - accuracy: 0.9698\n",
            "Epoch 191: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1239 - accuracy: 0.9704 - val_loss: 0.2976 - val_accuracy: 0.9439\n",
            "Epoch 192/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1078 - accuracy: 0.9758\n",
            "Epoch 192: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1046 - accuracy: 0.9757 - val_loss: 0.2946 - val_accuracy: 0.9479\n",
            "Epoch 193/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1412 - accuracy: 0.9719\n",
            "Epoch 193: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1424 - accuracy: 0.9717 - val_loss: 0.3161 - val_accuracy: 0.9456\n",
            "Epoch 194/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1148 - accuracy: 0.9663\n",
            "Epoch 194: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1116 - accuracy: 0.9669 - val_loss: 0.2986 - val_accuracy: 0.9439\n",
            "Epoch 195/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.0865 - accuracy: 0.9737\n",
            "Epoch 195: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0903 - accuracy: 0.9741 - val_loss: 0.2907 - val_accuracy: 0.9439\n",
            "Epoch 196/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.1283 - accuracy: 0.9719\n",
            "Epoch 196: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1282 - accuracy: 0.9719 - val_loss: 0.3081 - val_accuracy: 0.9439\n",
            "Epoch 197/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.1134 - accuracy: 0.9702\n",
            "Epoch 197: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1135 - accuracy: 0.9702 - val_loss: 0.2959 - val_accuracy: 0.9445\n",
            "Epoch 198/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1248 - accuracy: 0.9695\n",
            "Epoch 198: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1223 - accuracy: 0.9696 - val_loss: 0.2942 - val_accuracy: 0.9422\n",
            "Epoch 199/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.1114 - accuracy: 0.9711\n",
            "Epoch 199: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1130 - accuracy: 0.9707 - val_loss: 0.3056 - val_accuracy: 0.9439\n",
            "Epoch 200/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1364 - accuracy: 0.9695\n",
            "Epoch 200: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1342 - accuracy: 0.9701 - val_loss: 0.2725 - val_accuracy: 0.9382\n",
            "Epoch 201/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.1349 - accuracy: 0.9688\n",
            "Epoch 201: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1344 - accuracy: 0.9686 - val_loss: 0.3289 - val_accuracy: 0.9433\n",
            "Epoch 202/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1319 - accuracy: 0.9638\n",
            "Epoch 202: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1298 - accuracy: 0.9646 - val_loss: 0.3536 - val_accuracy: 0.9405\n",
            "Epoch 203/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1337 - accuracy: 0.9669\n",
            "Epoch 203: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1348 - accuracy: 0.9659 - val_loss: 0.3144 - val_accuracy: 0.9422\n",
            "Epoch 204/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1060 - accuracy: 0.9725\n",
            "Epoch 204: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1089 - accuracy: 0.9725 - val_loss: 0.2989 - val_accuracy: 0.9405\n",
            "Epoch 205/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1481 - accuracy: 0.9731\n",
            "Epoch 205: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1460 - accuracy: 0.9728 - val_loss: 0.3448 - val_accuracy: 0.9382\n",
            "Epoch 206/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.1978 - accuracy: 0.9692\n",
            "Epoch 206: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.2028 - accuracy: 0.9688 - val_loss: 0.3264 - val_accuracy: 0.9376\n",
            "Epoch 207/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1253 - accuracy: 0.9697\n",
            "Epoch 207: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1265 - accuracy: 0.9684 - val_loss: 0.3651 - val_accuracy: 0.9336\n",
            "Epoch 208/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.1099 - accuracy: 0.9720\n",
            "Epoch 208: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1106 - accuracy: 0.9719 - val_loss: 0.3244 - val_accuracy: 0.9433\n",
            "Epoch 209/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.0957 - accuracy: 0.9751\n",
            "Epoch 209: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0966 - accuracy: 0.9749 - val_loss: 0.3239 - val_accuracy: 0.9416\n",
            "Epoch 210/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1148 - accuracy: 0.9726\n",
            "Epoch 210: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1191 - accuracy: 0.9725 - val_loss: 0.3424 - val_accuracy: 0.9399\n",
            "Epoch 211/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1329 - accuracy: 0.9695\n",
            "Epoch 211: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1325 - accuracy: 0.9688 - val_loss: 0.3417 - val_accuracy: 0.9399\n",
            "Epoch 212/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.0970 - accuracy: 0.9730\n",
            "Epoch 212: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1083 - accuracy: 0.9721 - val_loss: 0.3320 - val_accuracy: 0.9399\n",
            "Epoch 213/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1255 - accuracy: 0.9709\n",
            "Epoch 213: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1224 - accuracy: 0.9715 - val_loss: 0.3176 - val_accuracy: 0.9410\n",
            "Epoch 214/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1278 - accuracy: 0.9709\n",
            "Epoch 214: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1260 - accuracy: 0.9708 - val_loss: 0.3335 - val_accuracy: 0.9416\n",
            "Epoch 215/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1126 - accuracy: 0.9721\n",
            "Epoch 215: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1121 - accuracy: 0.9719 - val_loss: 0.3403 - val_accuracy: 0.9376\n",
            "Epoch 216/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1147 - accuracy: 0.9716\n",
            "Epoch 216: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1189 - accuracy: 0.9711 - val_loss: 0.3128 - val_accuracy: 0.9428\n",
            "Epoch 217/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1110 - accuracy: 0.9704\n",
            "Epoch 217: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1106 - accuracy: 0.9695 - val_loss: 0.3436 - val_accuracy: 0.9399\n",
            "Epoch 218/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.1083 - accuracy: 0.9738\n",
            "Epoch 218: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1090 - accuracy: 0.9727 - val_loss: 0.3394 - val_accuracy: 0.9388\n",
            "Epoch 219/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1263 - accuracy: 0.9681\n",
            "Epoch 219: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1272 - accuracy: 0.9688 - val_loss: 0.3278 - val_accuracy: 0.9416\n",
            "Epoch 220/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1257 - accuracy: 0.9712\n",
            "Epoch 220: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1251 - accuracy: 0.9712 - val_loss: 0.3186 - val_accuracy: 0.9405\n",
            "Epoch 221/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.1135 - accuracy: 0.9744\n",
            "Epoch 221: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1123 - accuracy: 0.9745 - val_loss: 0.3652 - val_accuracy: 0.9410\n",
            "Epoch 222/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1088 - accuracy: 0.9722\n",
            "Epoch 222: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1302 - accuracy: 0.9715 - val_loss: 0.3090 - val_accuracy: 0.9353\n",
            "Epoch 223/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1200 - accuracy: 0.9691\n",
            "Epoch 223: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1171 - accuracy: 0.9694 - val_loss: 0.3017 - val_accuracy: 0.9439\n",
            "Epoch 224/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1021 - accuracy: 0.9729\n",
            "Epoch 224: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1084 - accuracy: 0.9729 - val_loss: 0.3113 - val_accuracy: 0.9473\n",
            "Epoch 225/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.0960 - accuracy: 0.9743\n",
            "Epoch 225: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0985 - accuracy: 0.9737 - val_loss: 0.3341 - val_accuracy: 0.9456\n",
            "Epoch 226/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.0984 - accuracy: 0.9767\n",
            "Epoch 226: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1032 - accuracy: 0.9758 - val_loss: 0.3568 - val_accuracy: 0.9473\n",
            "Epoch 227/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1059 - accuracy: 0.9718\n",
            "Epoch 227: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1081 - accuracy: 0.9711 - val_loss: 0.3446 - val_accuracy: 0.9456\n",
            "Epoch 228/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1077 - accuracy: 0.9749\n",
            "Epoch 228: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1053 - accuracy: 0.9749 - val_loss: 0.3410 - val_accuracy: 0.9462\n",
            "Epoch 229/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1177 - accuracy: 0.9746\n",
            "Epoch 229: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1156 - accuracy: 0.9745 - val_loss: 0.3523 - val_accuracy: 0.9479\n",
            "Epoch 230/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1135 - accuracy: 0.9730\n",
            "Epoch 230: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1135 - accuracy: 0.9727 - val_loss: 0.3657 - val_accuracy: 0.9433\n",
            "Epoch 231/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1074 - accuracy: 0.9745\n",
            "Epoch 231: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1058 - accuracy: 0.9744 - val_loss: 0.3671 - val_accuracy: 0.9433\n",
            "Epoch 232/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.0972 - accuracy: 0.9766\n",
            "Epoch 232: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0930 - accuracy: 0.9772 - val_loss: 0.3342 - val_accuracy: 0.9410\n",
            "Epoch 233/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.0991 - accuracy: 0.9758\n",
            "Epoch 233: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1029 - accuracy: 0.9751 - val_loss: 0.3424 - val_accuracy: 0.9456\n",
            "Epoch 234/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1116 - accuracy: 0.9776\n",
            "Epoch 234: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1084 - accuracy: 0.9778 - val_loss: 0.3510 - val_accuracy: 0.9422\n",
            "Epoch 235/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1217 - accuracy: 0.9732\n",
            "Epoch 235: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1248 - accuracy: 0.9728 - val_loss: 0.3468 - val_accuracy: 0.9382\n",
            "Epoch 236/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1118 - accuracy: 0.9739\n",
            "Epoch 236: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1071 - accuracy: 0.9739 - val_loss: 0.3162 - val_accuracy: 0.9450\n",
            "Epoch 237/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1088 - accuracy: 0.9728\n",
            "Epoch 237: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1062 - accuracy: 0.9729 - val_loss: 0.3265 - val_accuracy: 0.9410\n",
            "Epoch 238/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1001 - accuracy: 0.9766\n",
            "Epoch 238: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1057 - accuracy: 0.9765 - val_loss: 0.3157 - val_accuracy: 0.9433\n",
            "Epoch 239/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1308 - accuracy: 0.9712\n",
            "Epoch 239: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1249 - accuracy: 0.9714 - val_loss: 0.3243 - val_accuracy: 0.9370\n",
            "Epoch 240/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1102 - accuracy: 0.9726\n",
            "Epoch 240: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1131 - accuracy: 0.9718 - val_loss: 0.3191 - val_accuracy: 0.9405\n",
            "Epoch 241/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1328 - accuracy: 0.9707\n",
            "Epoch 241: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1305 - accuracy: 0.9707 - val_loss: 0.3201 - val_accuracy: 0.9399\n",
            "Epoch 242/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1127 - accuracy: 0.9721\n",
            "Epoch 242: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1137 - accuracy: 0.9715 - val_loss: 0.3018 - val_accuracy: 0.9439\n",
            "Epoch 243/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1069 - accuracy: 0.9718\n",
            "Epoch 243: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1076 - accuracy: 0.9714 - val_loss: 0.3225 - val_accuracy: 0.9445\n",
            "Epoch 244/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.0987 - accuracy: 0.9758\n",
            "Epoch 244: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0973 - accuracy: 0.9757 - val_loss: 0.3151 - val_accuracy: 0.9416\n",
            "Epoch 245/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1247 - accuracy: 0.9722\n",
            "Epoch 245: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1210 - accuracy: 0.9721 - val_loss: 0.3257 - val_accuracy: 0.9433\n",
            "Epoch 246/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1055 - accuracy: 0.9722\n",
            "Epoch 246: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1029 - accuracy: 0.9731 - val_loss: 0.3257 - val_accuracy: 0.9456\n",
            "Epoch 247/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.0982 - accuracy: 0.9750\n",
            "Epoch 247: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0961 - accuracy: 0.9749 - val_loss: 0.3257 - val_accuracy: 0.9479\n",
            "Epoch 248/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1224 - accuracy: 0.9727\n",
            "Epoch 248: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1268 - accuracy: 0.9715 - val_loss: 0.3146 - val_accuracy: 0.9450\n",
            "Epoch 249/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1008 - accuracy: 0.9719\n",
            "Epoch 249: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0982 - accuracy: 0.9724 - val_loss: 0.3353 - val_accuracy: 0.9365\n",
            "Epoch 250/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.0972 - accuracy: 0.9730\n",
            "Epoch 250: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1043 - accuracy: 0.9728 - val_loss: 0.3264 - val_accuracy: 0.9399\n",
            "Epoch 251/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.0883 - accuracy: 0.9761\n",
            "Epoch 251: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0903 - accuracy: 0.9767 - val_loss: 0.3485 - val_accuracy: 0.9496\n",
            "Epoch 252/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.0802 - accuracy: 0.9797\n",
            "Epoch 252: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0846 - accuracy: 0.9792 - val_loss: 0.3147 - val_accuracy: 0.9462\n",
            "Epoch 253/500\n",
            "53/55 [===========================>..] - ETA: 0s - loss: 0.1107 - accuracy: 0.9717\n",
            "Epoch 253: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1092 - accuracy: 0.9719 - val_loss: 0.3321 - val_accuracy: 0.9416\n",
            "Epoch 254/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1061 - accuracy: 0.9729\n",
            "Epoch 254: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1085 - accuracy: 0.9719 - val_loss: 0.3399 - val_accuracy: 0.9468\n",
            "Epoch 255/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1101 - accuracy: 0.9751\n",
            "Epoch 255: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1092 - accuracy: 0.9747 - val_loss: 0.3379 - val_accuracy: 0.9479\n",
            "Epoch 256/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.0877 - accuracy: 0.9783\n",
            "Epoch 256: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0884 - accuracy: 0.9785 - val_loss: 0.3166 - val_accuracy: 0.9433\n",
            "Epoch 257/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1034 - accuracy: 0.9716\n",
            "Epoch 257: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1033 - accuracy: 0.9722 - val_loss: 0.3430 - val_accuracy: 0.9473\n",
            "Epoch 258/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.0934 - accuracy: 0.9740\n",
            "Epoch 258: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1010 - accuracy: 0.9745 - val_loss: 0.3416 - val_accuracy: 0.9485\n",
            "Epoch 259/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1031 - accuracy: 0.9742\n",
            "Epoch 259: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1083 - accuracy: 0.9735 - val_loss: 0.3264 - val_accuracy: 0.9513\n",
            "Epoch 260/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1144 - accuracy: 0.9692\n",
            "Epoch 260: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1155 - accuracy: 0.9688 - val_loss: 0.3115 - val_accuracy: 0.9473\n",
            "Epoch 261/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.0933 - accuracy: 0.9722\n",
            "Epoch 261: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1034 - accuracy: 0.9717 - val_loss: 0.3206 - val_accuracy: 0.9502\n",
            "Epoch 262/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1292 - accuracy: 0.9706\n",
            "Epoch 262: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1280 - accuracy: 0.9709 - val_loss: 0.3563 - val_accuracy: 0.9416\n",
            "Epoch 263/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.0993 - accuracy: 0.9737\n",
            "Epoch 263: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1055 - accuracy: 0.9737 - val_loss: 0.3533 - val_accuracy: 0.9410\n",
            "Epoch 264/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.1079 - accuracy: 0.9767\n",
            "Epoch 264: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1100 - accuracy: 0.9758 - val_loss: 0.3568 - val_accuracy: 0.9382\n",
            "Epoch 265/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1223 - accuracy: 0.9727\n",
            "Epoch 265: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1328 - accuracy: 0.9725 - val_loss: 0.3141 - val_accuracy: 0.9399\n",
            "Epoch 266/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1072 - accuracy: 0.9727\n",
            "Epoch 266: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1113 - accuracy: 0.9719 - val_loss: 0.3350 - val_accuracy: 0.9502\n",
            "Epoch 267/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1155 - accuracy: 0.9700\n",
            "Epoch 267: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1104 - accuracy: 0.9709 - val_loss: 0.3424 - val_accuracy: 0.9462\n",
            "Epoch 268/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1185 - accuracy: 0.9746\n",
            "Epoch 268: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1271 - accuracy: 0.9739 - val_loss: 0.3398 - val_accuracy: 0.9462\n",
            "Epoch 269/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1376 - accuracy: 0.9710\n",
            "Epoch 269: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1349 - accuracy: 0.9714 - val_loss: 0.3635 - val_accuracy: 0.9468\n",
            "Epoch 270/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.0891 - accuracy: 0.9762\n",
            "Epoch 270: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0864 - accuracy: 0.9761 - val_loss: 0.3624 - val_accuracy: 0.9410\n",
            "Epoch 271/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1096 - accuracy: 0.9744\n",
            "Epoch 271: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1084 - accuracy: 0.9742 - val_loss: 0.3617 - val_accuracy: 0.9376\n",
            "Epoch 272/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1034 - accuracy: 0.9733\n",
            "Epoch 272: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1070 - accuracy: 0.9732 - val_loss: 0.3326 - val_accuracy: 0.9439\n",
            "Epoch 273/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1299 - accuracy: 0.9767\n",
            "Epoch 273: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1311 - accuracy: 0.9757 - val_loss: 0.3299 - val_accuracy: 0.9439\n",
            "Epoch 274/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1187 - accuracy: 0.9748\n",
            "Epoch 274: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1183 - accuracy: 0.9752 - val_loss: 0.3668 - val_accuracy: 0.9422\n",
            "Epoch 275/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1127 - accuracy: 0.9737\n",
            "Epoch 275: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1072 - accuracy: 0.9747 - val_loss: 0.3937 - val_accuracy: 0.9376\n",
            "Epoch 276/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1234 - accuracy: 0.9723\n",
            "Epoch 276: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1213 - accuracy: 0.9725 - val_loss: 0.3618 - val_accuracy: 0.9439\n",
            "Epoch 277/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1040 - accuracy: 0.9749\n",
            "Epoch 277: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1024 - accuracy: 0.9748 - val_loss: 0.3445 - val_accuracy: 0.9479\n",
            "Epoch 278/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1029 - accuracy: 0.9737\n",
            "Epoch 278: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1071 - accuracy: 0.9729 - val_loss: 0.3626 - val_accuracy: 0.9410\n",
            "Epoch 279/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.0924 - accuracy: 0.9784\n",
            "Epoch 279: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0917 - accuracy: 0.9784 - val_loss: 0.3508 - val_accuracy: 0.9410\n",
            "Epoch 280/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1142 - accuracy: 0.9739\n",
            "Epoch 280: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1120 - accuracy: 0.9742 - val_loss: 0.3288 - val_accuracy: 0.9491\n",
            "Epoch 281/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1091 - accuracy: 0.9740\n",
            "Epoch 281: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1092 - accuracy: 0.9738 - val_loss: 0.2979 - val_accuracy: 0.9468\n",
            "Epoch 282/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1208 - accuracy: 0.9718\n",
            "Epoch 282: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1219 - accuracy: 0.9708 - val_loss: 0.3232 - val_accuracy: 0.9450\n",
            "Epoch 283/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.0946 - accuracy: 0.9736\n",
            "Epoch 283: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0989 - accuracy: 0.9734 - val_loss: 0.3341 - val_accuracy: 0.9445\n",
            "Epoch 284/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.1171 - accuracy: 0.9736\n",
            "Epoch 284: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1151 - accuracy: 0.9745 - val_loss: 0.3374 - val_accuracy: 0.9428\n",
            "Epoch 285/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.0978 - accuracy: 0.9731\n",
            "Epoch 285: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1009 - accuracy: 0.9735 - val_loss: 0.3273 - val_accuracy: 0.9450\n",
            "Epoch 286/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.0946 - accuracy: 0.9781\n",
            "Epoch 286: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0916 - accuracy: 0.9782 - val_loss: 0.3458 - val_accuracy: 0.9456\n",
            "Epoch 287/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1213 - accuracy: 0.9741\n",
            "Epoch 287: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1169 - accuracy: 0.9745 - val_loss: 0.3455 - val_accuracy: 0.9399\n",
            "Epoch 288/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.0911 - accuracy: 0.9737\n",
            "Epoch 288: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0946 - accuracy: 0.9737 - val_loss: 0.3263 - val_accuracy: 0.9445\n",
            "Epoch 289/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.0853 - accuracy: 0.9794\n",
            "Epoch 289: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0829 - accuracy: 0.9795 - val_loss: 0.3039 - val_accuracy: 0.9462\n",
            "Epoch 290/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.0829 - accuracy: 0.9788\n",
            "Epoch 290: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0818 - accuracy: 0.9787 - val_loss: 0.3161 - val_accuracy: 0.9439\n",
            "Epoch 291/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1196 - accuracy: 0.9755\n",
            "Epoch 291: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1247 - accuracy: 0.9751 - val_loss: 0.3236 - val_accuracy: 0.9491\n",
            "Epoch 292/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1170 - accuracy: 0.9717\n",
            "Epoch 292: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1166 - accuracy: 0.9714 - val_loss: 0.3278 - val_accuracy: 0.9462\n",
            "Epoch 293/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1191 - accuracy: 0.9715\n",
            "Epoch 293: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1148 - accuracy: 0.9721 - val_loss: 0.3216 - val_accuracy: 0.9491\n",
            "Epoch 294/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.0819 - accuracy: 0.9778\n",
            "Epoch 294: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0857 - accuracy: 0.9772 - val_loss: 0.3736 - val_accuracy: 0.9462\n",
            "Epoch 295/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.0973 - accuracy: 0.9774\n",
            "Epoch 295: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0999 - accuracy: 0.9774 - val_loss: 0.3364 - val_accuracy: 0.9450\n",
            "Epoch 296/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.0882 - accuracy: 0.9755\n",
            "Epoch 296: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0885 - accuracy: 0.9752 - val_loss: 0.3518 - val_accuracy: 0.9491\n",
            "Epoch 297/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1086 - accuracy: 0.9733\n",
            "Epoch 297: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1066 - accuracy: 0.9735 - val_loss: 0.3443 - val_accuracy: 0.9450\n",
            "Epoch 298/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1244 - accuracy: 0.9749\n",
            "Epoch 298: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1190 - accuracy: 0.9744 - val_loss: 0.3584 - val_accuracy: 0.9405\n",
            "Epoch 299/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.0904 - accuracy: 0.9753\n",
            "Epoch 299: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0919 - accuracy: 0.9745 - val_loss: 0.3709 - val_accuracy: 0.9376\n",
            "Epoch 300/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1173 - accuracy: 0.9742\n",
            "Epoch 300: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1166 - accuracy: 0.9747 - val_loss: 0.3793 - val_accuracy: 0.9393\n",
            "Epoch 301/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.0980 - accuracy: 0.9741\n",
            "Epoch 301: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0995 - accuracy: 0.9738 - val_loss: 0.3987 - val_accuracy: 0.9393\n",
            "Epoch 302/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1114 - accuracy: 0.9751\n",
            "Epoch 302: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1091 - accuracy: 0.9745 - val_loss: 0.3430 - val_accuracy: 0.9445\n",
            "Epoch 303/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.0995 - accuracy: 0.9764\n",
            "Epoch 303: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1074 - accuracy: 0.9758 - val_loss: 0.3668 - val_accuracy: 0.9399\n",
            "Epoch 304/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1255 - accuracy: 0.9728\n",
            "Epoch 304: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1212 - accuracy: 0.9728 - val_loss: 0.3929 - val_accuracy: 0.9376\n",
            "Epoch 305/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1023 - accuracy: 0.9731\n",
            "Epoch 305: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1002 - accuracy: 0.9741 - val_loss: 0.4076 - val_accuracy: 0.9416\n",
            "Epoch 306/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1273 - accuracy: 0.9717\n",
            "Epoch 306: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1229 - accuracy: 0.9715 - val_loss: 0.4233 - val_accuracy: 0.9428\n",
            "Epoch 307/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.0984 - accuracy: 0.9751\n",
            "Epoch 307: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0969 - accuracy: 0.9745 - val_loss: 0.3639 - val_accuracy: 0.9416\n",
            "Epoch 308/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1124 - accuracy: 0.9728\n",
            "Epoch 308: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1106 - accuracy: 0.9732 - val_loss: 0.3805 - val_accuracy: 0.9405\n",
            "Epoch 309/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1249 - accuracy: 0.9761\n",
            "Epoch 309: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1198 - accuracy: 0.9757 - val_loss: 0.3571 - val_accuracy: 0.9399\n",
            "Epoch 310/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1375 - accuracy: 0.9707\n",
            "Epoch 310: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1449 - accuracy: 0.9704 - val_loss: 0.3465 - val_accuracy: 0.9405\n",
            "Epoch 311/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1604 - accuracy: 0.9665\n",
            "Epoch 311: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1525 - accuracy: 0.9674 - val_loss: 0.3475 - val_accuracy: 0.9376\n",
            "Epoch 312/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1423 - accuracy: 0.9707\n",
            "Epoch 312: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1491 - accuracy: 0.9699 - val_loss: 0.3431 - val_accuracy: 0.9405\n",
            "Epoch 313/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.1260 - accuracy: 0.9693\n",
            "Epoch 313: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1235 - accuracy: 0.9711 - val_loss: 0.3706 - val_accuracy: 0.9388\n",
            "Epoch 314/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1101 - accuracy: 0.9709\n",
            "Epoch 314: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1116 - accuracy: 0.9699 - val_loss: 0.3873 - val_accuracy: 0.9405\n",
            "Epoch 315/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.1732 - accuracy: 0.9694\n",
            "Epoch 315: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1614 - accuracy: 0.9708 - val_loss: 0.3756 - val_accuracy: 0.9325\n",
            "Epoch 316/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1280 - accuracy: 0.9719\n",
            "Epoch 316: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1256 - accuracy: 0.9719 - val_loss: 0.3772 - val_accuracy: 0.9399\n",
            "Epoch 317/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1621 - accuracy: 0.9692\n",
            "Epoch 317: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1639 - accuracy: 0.9692 - val_loss: 0.3666 - val_accuracy: 0.9422\n",
            "Epoch 318/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1268 - accuracy: 0.9675\n",
            "Epoch 318: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1266 - accuracy: 0.9674 - val_loss: 0.3697 - val_accuracy: 0.9428\n",
            "Epoch 319/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1159 - accuracy: 0.9748\n",
            "Epoch 319: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1166 - accuracy: 0.9744 - val_loss: 0.3832 - val_accuracy: 0.9382\n",
            "Epoch 320/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1252 - accuracy: 0.9712\n",
            "Epoch 320: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1226 - accuracy: 0.9712 - val_loss: 0.3728 - val_accuracy: 0.9410\n",
            "Epoch 321/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1135 - accuracy: 0.9691\n",
            "Epoch 321: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1161 - accuracy: 0.9688 - val_loss: 0.3737 - val_accuracy: 0.9422\n",
            "Epoch 322/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.0852 - accuracy: 0.9748\n",
            "Epoch 322: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0854 - accuracy: 0.9748 - val_loss: 0.3927 - val_accuracy: 0.9405\n",
            "Epoch 323/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.0999 - accuracy: 0.9770\n",
            "Epoch 323: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0993 - accuracy: 0.9772 - val_loss: 0.3845 - val_accuracy: 0.9416\n",
            "Epoch 324/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1109 - accuracy: 0.9743\n",
            "Epoch 324: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1058 - accuracy: 0.9754 - val_loss: 0.4148 - val_accuracy: 0.9365\n",
            "Epoch 325/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1351 - accuracy: 0.9728\n",
            "Epoch 325: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1305 - accuracy: 0.9727 - val_loss: 0.4322 - val_accuracy: 0.9365\n",
            "Epoch 326/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1290 - accuracy: 0.9679\n",
            "Epoch 326: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1246 - accuracy: 0.9694 - val_loss: 0.4280 - val_accuracy: 0.9376\n",
            "Epoch 327/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1148 - accuracy: 0.9727\n",
            "Epoch 327: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1159 - accuracy: 0.9729 - val_loss: 0.4420 - val_accuracy: 0.9393\n",
            "Epoch 328/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1080 - accuracy: 0.9748\n",
            "Epoch 328: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1029 - accuracy: 0.9758 - val_loss: 0.4423 - val_accuracy: 0.9428\n",
            "Epoch 329/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1346 - accuracy: 0.9691\n",
            "Epoch 329: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1285 - accuracy: 0.9696 - val_loss: 0.4236 - val_accuracy: 0.9393\n",
            "Epoch 330/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1284 - accuracy: 0.9696\n",
            "Epoch 330: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1342 - accuracy: 0.9691 - val_loss: 0.4542 - val_accuracy: 0.9382\n",
            "Epoch 331/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1410 - accuracy: 0.9705\n",
            "Epoch 331: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1360 - accuracy: 0.9704 - val_loss: 0.4092 - val_accuracy: 0.9410\n",
            "Epoch 332/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1054 - accuracy: 0.9740\n",
            "Epoch 332: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1020 - accuracy: 0.9748 - val_loss: 0.4254 - val_accuracy: 0.9388\n",
            "Epoch 333/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1063 - accuracy: 0.9762\n",
            "Epoch 333: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1117 - accuracy: 0.9757 - val_loss: 0.4310 - val_accuracy: 0.9416\n",
            "Epoch 334/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1078 - accuracy: 0.9746\n",
            "Epoch 334: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1021 - accuracy: 0.9754 - val_loss: 0.4085 - val_accuracy: 0.9393\n",
            "Epoch 335/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.1305 - accuracy: 0.9761\n",
            "Epoch 335: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1278 - accuracy: 0.9757 - val_loss: 0.3980 - val_accuracy: 0.9405\n",
            "Epoch 336/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.0999 - accuracy: 0.9753\n",
            "Epoch 336: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1000 - accuracy: 0.9754 - val_loss: 0.3804 - val_accuracy: 0.9399\n",
            "Epoch 337/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1195 - accuracy: 0.9692\n",
            "Epoch 337: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1166 - accuracy: 0.9698 - val_loss: 0.3729 - val_accuracy: 0.9439\n",
            "Epoch 338/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.0920 - accuracy: 0.9767\n",
            "Epoch 338: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0896 - accuracy: 0.9774 - val_loss: 0.3919 - val_accuracy: 0.9450\n",
            "Epoch 339/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.0980 - accuracy: 0.9779\n",
            "Epoch 339: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0923 - accuracy: 0.9781 - val_loss: 0.4357 - val_accuracy: 0.9410\n",
            "Epoch 340/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.0748 - accuracy: 0.9812\n",
            "Epoch 340: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0714 - accuracy: 0.9820 - val_loss: 0.3989 - val_accuracy: 0.9388\n",
            "Epoch 341/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1091 - accuracy: 0.9794\n",
            "Epoch 341: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1073 - accuracy: 0.9790 - val_loss: 0.3849 - val_accuracy: 0.9439\n",
            "Epoch 342/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1172 - accuracy: 0.9743\n",
            "Epoch 342: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1216 - accuracy: 0.9737 - val_loss: 0.3638 - val_accuracy: 0.9370\n",
            "Epoch 343/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1077 - accuracy: 0.9726\n",
            "Epoch 343: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1044 - accuracy: 0.9737 - val_loss: 0.3441 - val_accuracy: 0.9468\n",
            "Epoch 344/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1196 - accuracy: 0.9769\n",
            "Epoch 344: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1144 - accuracy: 0.9771 - val_loss: 0.3662 - val_accuracy: 0.9428\n",
            "Epoch 345/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.0863 - accuracy: 0.9771\n",
            "Epoch 345: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0902 - accuracy: 0.9764 - val_loss: 0.3599 - val_accuracy: 0.9410\n",
            "Epoch 346/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1041 - accuracy: 0.9786\n",
            "Epoch 346: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1086 - accuracy: 0.9775 - val_loss: 0.3713 - val_accuracy: 0.9479\n",
            "Epoch 347/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1015 - accuracy: 0.9753\n",
            "Epoch 347: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1070 - accuracy: 0.9751 - val_loss: 0.3640 - val_accuracy: 0.9416\n",
            "Epoch 348/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.0940 - accuracy: 0.9766\n",
            "Epoch 348: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0961 - accuracy: 0.9761 - val_loss: 0.3957 - val_accuracy: 0.9422\n",
            "Epoch 349/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1269 - accuracy: 0.9703\n",
            "Epoch 349: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1254 - accuracy: 0.9704 - val_loss: 0.3814 - val_accuracy: 0.9439\n",
            "Epoch 350/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1231 - accuracy: 0.9702\n",
            "Epoch 350: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1250 - accuracy: 0.9711 - val_loss: 0.3815 - val_accuracy: 0.9416\n",
            "Epoch 351/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1216 - accuracy: 0.9716\n",
            "Epoch 351: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1203 - accuracy: 0.9714 - val_loss: 0.3880 - val_accuracy: 0.9399\n",
            "Epoch 352/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1092 - accuracy: 0.9767\n",
            "Epoch 352: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1087 - accuracy: 0.9758 - val_loss: 0.3909 - val_accuracy: 0.9410\n",
            "Epoch 353/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.0832 - accuracy: 0.9775\n",
            "Epoch 353: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0900 - accuracy: 0.9777 - val_loss: 0.3852 - val_accuracy: 0.9456\n",
            "Epoch 354/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.0692 - accuracy: 0.9784\n",
            "Epoch 354: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0724 - accuracy: 0.9795 - val_loss: 0.4134 - val_accuracy: 0.9410\n",
            "Epoch 355/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1247 - accuracy: 0.9776\n",
            "Epoch 355: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1221 - accuracy: 0.9778 - val_loss: 0.3910 - val_accuracy: 0.9462\n",
            "Epoch 356/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1021 - accuracy: 0.9743\n",
            "Epoch 356: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1064 - accuracy: 0.9738 - val_loss: 0.3677 - val_accuracy: 0.9393\n",
            "Epoch 357/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.0866 - accuracy: 0.9767\n",
            "Epoch 357: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0879 - accuracy: 0.9765 - val_loss: 0.4403 - val_accuracy: 0.9410\n",
            "Epoch 358/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.1086 - accuracy: 0.9745\n",
            "Epoch 358: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1032 - accuracy: 0.9749 - val_loss: 0.4551 - val_accuracy: 0.9405\n",
            "Epoch 359/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1113 - accuracy: 0.9756\n",
            "Epoch 359: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1197 - accuracy: 0.9752 - val_loss: 0.4184 - val_accuracy: 0.9485\n",
            "Epoch 360/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1349 - accuracy: 0.9711\n",
            "Epoch 360: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1281 - accuracy: 0.9715 - val_loss: 0.4166 - val_accuracy: 0.9410\n",
            "Epoch 361/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1062 - accuracy: 0.9748\n",
            "Epoch 361: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1078 - accuracy: 0.9747 - val_loss: 0.4238 - val_accuracy: 0.9399\n",
            "Epoch 362/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1068 - accuracy: 0.9735\n",
            "Epoch 362: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1065 - accuracy: 0.9734 - val_loss: 0.4331 - val_accuracy: 0.9462\n",
            "Epoch 363/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1133 - accuracy: 0.9696\n",
            "Epoch 363: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1121 - accuracy: 0.9702 - val_loss: 0.4474 - val_accuracy: 0.9388\n",
            "Epoch 364/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1215 - accuracy: 0.9715\n",
            "Epoch 364: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1213 - accuracy: 0.9714 - val_loss: 0.4071 - val_accuracy: 0.9382\n",
            "Epoch 365/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1251 - accuracy: 0.9735\n",
            "Epoch 365: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1239 - accuracy: 0.9732 - val_loss: 0.4050 - val_accuracy: 0.9405\n",
            "Epoch 366/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1217 - accuracy: 0.9711\n",
            "Epoch 366: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1279 - accuracy: 0.9704 - val_loss: 0.4125 - val_accuracy: 0.9382\n",
            "Epoch 367/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1380 - accuracy: 0.9697\n",
            "Epoch 367: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1444 - accuracy: 0.9686 - val_loss: 0.4257 - val_accuracy: 0.9439\n",
            "Epoch 368/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1186 - accuracy: 0.9746\n",
            "Epoch 368: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1122 - accuracy: 0.9755 - val_loss: 0.4216 - val_accuracy: 0.9462\n",
            "Epoch 369/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1145 - accuracy: 0.9730\n",
            "Epoch 369: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1134 - accuracy: 0.9724 - val_loss: 0.4557 - val_accuracy: 0.9462\n",
            "Epoch 370/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1383 - accuracy: 0.9706\n",
            "Epoch 370: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1436 - accuracy: 0.9702 - val_loss: 0.3866 - val_accuracy: 0.9405\n",
            "Epoch 371/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1236 - accuracy: 0.9700\n",
            "Epoch 371: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1154 - accuracy: 0.9721 - val_loss: 0.3754 - val_accuracy: 0.9456\n",
            "Epoch 372/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.0906 - accuracy: 0.9799\n",
            "Epoch 372: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0970 - accuracy: 0.9788 - val_loss: 0.4200 - val_accuracy: 0.9485\n",
            "Epoch 373/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1070 - accuracy: 0.9766\n",
            "Epoch 373: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1042 - accuracy: 0.9771 - val_loss: 0.3739 - val_accuracy: 0.9508\n",
            "Epoch 374/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1298 - accuracy: 0.9748\n",
            "Epoch 374: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 8ms/step - loss: 0.1263 - accuracy: 0.9751 - val_loss: 0.3936 - val_accuracy: 0.9479\n",
            "Epoch 375/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1262 - accuracy: 0.9717\n",
            "Epoch 375: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 1s 9ms/step - loss: 0.1238 - accuracy: 0.9715 - val_loss: 0.3570 - val_accuracy: 0.9456\n",
            "Epoch 376/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.1368 - accuracy: 0.9735\n",
            "Epoch 376: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1274 - accuracy: 0.9739 - val_loss: 0.4169 - val_accuracy: 0.9445\n",
            "Epoch 377/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.0945 - accuracy: 0.9764\n",
            "Epoch 377: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0927 - accuracy: 0.9767 - val_loss: 0.4323 - val_accuracy: 0.9445\n",
            "Epoch 378/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1106 - accuracy: 0.9748\n",
            "Epoch 378: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1098 - accuracy: 0.9741 - val_loss: 0.4067 - val_accuracy: 0.9468\n",
            "Epoch 379/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1304 - accuracy: 0.9727\n",
            "Epoch 379: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1212 - accuracy: 0.9738 - val_loss: 0.4325 - val_accuracy: 0.9433\n",
            "Epoch 380/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1423 - accuracy: 0.9730\n",
            "Epoch 380: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1515 - accuracy: 0.9717 - val_loss: 0.3840 - val_accuracy: 0.9388\n",
            "Epoch 381/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1361 - accuracy: 0.9707\n",
            "Epoch 381: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1322 - accuracy: 0.9711 - val_loss: 0.4110 - val_accuracy: 0.9388\n",
            "Epoch 382/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1139 - accuracy: 0.9734\n",
            "Epoch 382: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1156 - accuracy: 0.9728 - val_loss: 0.3845 - val_accuracy: 0.9353\n",
            "Epoch 383/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1094 - accuracy: 0.9736\n",
            "Epoch 383: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1085 - accuracy: 0.9744 - val_loss: 0.3556 - val_accuracy: 0.9393\n",
            "Epoch 384/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.1031 - accuracy: 0.9747\n",
            "Epoch 384: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0981 - accuracy: 0.9752 - val_loss: 0.3443 - val_accuracy: 0.9468\n",
            "Epoch 385/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.0897 - accuracy: 0.9748\n",
            "Epoch 385: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0936 - accuracy: 0.9745 - val_loss: 0.3969 - val_accuracy: 0.9433\n",
            "Epoch 386/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.0943 - accuracy: 0.9779\n",
            "Epoch 386: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0984 - accuracy: 0.9768 - val_loss: 0.4008 - val_accuracy: 0.9468\n",
            "Epoch 387/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1232 - accuracy: 0.9734\n",
            "Epoch 387: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1166 - accuracy: 0.9742 - val_loss: 0.4104 - val_accuracy: 0.9433\n",
            "Epoch 388/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1241 - accuracy: 0.9758\n",
            "Epoch 388: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1173 - accuracy: 0.9761 - val_loss: 0.4027 - val_accuracy: 0.9531\n",
            "Epoch 389/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1041 - accuracy: 0.9769\n",
            "Epoch 389: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1022 - accuracy: 0.9761 - val_loss: 0.4406 - val_accuracy: 0.9422\n",
            "Epoch 390/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.1237 - accuracy: 0.9721\n",
            "Epoch 390: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1165 - accuracy: 0.9724 - val_loss: 0.4290 - val_accuracy: 0.9405\n",
            "Epoch 391/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1317 - accuracy: 0.9739\n",
            "Epoch 391: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1279 - accuracy: 0.9742 - val_loss: 0.4009 - val_accuracy: 0.9416\n",
            "Epoch 392/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1047 - accuracy: 0.9757\n",
            "Epoch 392: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1121 - accuracy: 0.9745 - val_loss: 0.3741 - val_accuracy: 0.9479\n",
            "Epoch 393/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1259 - accuracy: 0.9783\n",
            "Epoch 393: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1225 - accuracy: 0.9782 - val_loss: 0.4024 - val_accuracy: 0.9450\n",
            "Epoch 394/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1011 - accuracy: 0.9782\n",
            "Epoch 394: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0986 - accuracy: 0.9784 - val_loss: 0.3702 - val_accuracy: 0.9445\n",
            "Epoch 395/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.0769 - accuracy: 0.9801\n",
            "Epoch 395: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0726 - accuracy: 0.9802 - val_loss: 0.3740 - val_accuracy: 0.9485\n",
            "Epoch 396/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.0781 - accuracy: 0.9790\n",
            "Epoch 396: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0776 - accuracy: 0.9792 - val_loss: 0.4068 - val_accuracy: 0.9496\n",
            "Epoch 397/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.0937 - accuracy: 0.9783\n",
            "Epoch 397: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0984 - accuracy: 0.9787 - val_loss: 0.4143 - val_accuracy: 0.9445\n",
            "Epoch 398/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.0955 - accuracy: 0.9741\n",
            "Epoch 398: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0968 - accuracy: 0.9741 - val_loss: 0.4219 - val_accuracy: 0.9439\n",
            "Epoch 399/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1223 - accuracy: 0.9731\n",
            "Epoch 399: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1232 - accuracy: 0.9737 - val_loss: 0.4191 - val_accuracy: 0.9450\n",
            "Epoch 400/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1320 - accuracy: 0.9723\n",
            "Epoch 400: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1264 - accuracy: 0.9725 - val_loss: 0.4137 - val_accuracy: 0.9462\n",
            "Epoch 401/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.0955 - accuracy: 0.9759\n",
            "Epoch 401: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0953 - accuracy: 0.9768 - val_loss: 0.4055 - val_accuracy: 0.9450\n",
            "Epoch 402/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1053 - accuracy: 0.9764\n",
            "Epoch 402: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1047 - accuracy: 0.9768 - val_loss: 0.4144 - val_accuracy: 0.9473\n",
            "Epoch 403/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1220 - accuracy: 0.9707\n",
            "Epoch 403: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1216 - accuracy: 0.9701 - val_loss: 0.3933 - val_accuracy: 0.9428\n",
            "Epoch 404/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1484 - accuracy: 0.9670\n",
            "Epoch 404: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1451 - accuracy: 0.9671 - val_loss: 0.4050 - val_accuracy: 0.9416\n",
            "Epoch 405/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.1252 - accuracy: 0.9716\n",
            "Epoch 405: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1225 - accuracy: 0.9718 - val_loss: 0.3975 - val_accuracy: 0.9336\n",
            "Epoch 406/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1053 - accuracy: 0.9732\n",
            "Epoch 406: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1078 - accuracy: 0.9727 - val_loss: 0.3829 - val_accuracy: 0.9393\n",
            "Epoch 407/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1032 - accuracy: 0.9745\n",
            "Epoch 407: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0969 - accuracy: 0.9757 - val_loss: 0.3956 - val_accuracy: 0.9450\n",
            "Epoch 408/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.9751\n",
            "Epoch 408: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0898 - accuracy: 0.9751 - val_loss: 0.3701 - val_accuracy: 0.9405\n",
            "Epoch 409/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1178 - accuracy: 0.9748\n",
            "Epoch 409: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1134 - accuracy: 0.9748 - val_loss: 0.3589 - val_accuracy: 0.9450\n",
            "Epoch 410/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.0985 - accuracy: 0.9734\n",
            "Epoch 410: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1015 - accuracy: 0.9739 - val_loss: 0.3703 - val_accuracy: 0.9416\n",
            "Epoch 411/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1088 - accuracy: 0.9749\n",
            "Epoch 411: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1038 - accuracy: 0.9749 - val_loss: 0.3459 - val_accuracy: 0.9416\n",
            "Epoch 412/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.1031 - accuracy: 0.9769\n",
            "Epoch 412: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1025 - accuracy: 0.9768 - val_loss: 0.3690 - val_accuracy: 0.9399\n",
            "Epoch 413/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1068 - accuracy: 0.9740\n",
            "Epoch 413: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1033 - accuracy: 0.9752 - val_loss: 0.3780 - val_accuracy: 0.9376\n",
            "Epoch 414/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1141 - accuracy: 0.9740\n",
            "Epoch 414: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1138 - accuracy: 0.9734 - val_loss: 0.4054 - val_accuracy: 0.9376\n",
            "Epoch 415/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1315 - accuracy: 0.9707\n",
            "Epoch 415: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1346 - accuracy: 0.9704 - val_loss: 0.3856 - val_accuracy: 0.9428\n",
            "Epoch 416/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.1305 - accuracy: 0.9703\n",
            "Epoch 416: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1321 - accuracy: 0.9711 - val_loss: 0.3624 - val_accuracy: 0.9428\n",
            "Epoch 417/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.0892 - accuracy: 0.9766\n",
            "Epoch 417: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1000 - accuracy: 0.9762 - val_loss: 0.3599 - val_accuracy: 0.9468\n",
            "Epoch 418/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1090 - accuracy: 0.9734\n",
            "Epoch 418: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1134 - accuracy: 0.9715 - val_loss: 0.4101 - val_accuracy: 0.9342\n",
            "Epoch 419/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1313 - accuracy: 0.9712\n",
            "Epoch 419: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1333 - accuracy: 0.9699 - val_loss: 0.3688 - val_accuracy: 0.9428\n",
            "Epoch 420/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.1039 - accuracy: 0.9752\n",
            "Epoch 420: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1018 - accuracy: 0.9745 - val_loss: 0.3981 - val_accuracy: 0.9439\n",
            "Epoch 421/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.1024 - accuracy: 0.9759\n",
            "Epoch 421: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1074 - accuracy: 0.9767 - val_loss: 0.3755 - val_accuracy: 0.9496\n",
            "Epoch 422/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1231 - accuracy: 0.9737\n",
            "Epoch 422: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1188 - accuracy: 0.9737 - val_loss: 0.3603 - val_accuracy: 0.9473\n",
            "Epoch 423/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1024 - accuracy: 0.9735\n",
            "Epoch 423: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1036 - accuracy: 0.9737 - val_loss: 0.3859 - val_accuracy: 0.9485\n",
            "Epoch 424/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1140 - accuracy: 0.9753\n",
            "Epoch 424: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1085 - accuracy: 0.9754 - val_loss: 0.3887 - val_accuracy: 0.9485\n",
            "Epoch 425/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1263 - accuracy: 0.9758\n",
            "Epoch 425: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1279 - accuracy: 0.9751 - val_loss: 0.4083 - val_accuracy: 0.9462\n",
            "Epoch 426/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1554 - accuracy: 0.9701\n",
            "Epoch 426: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1614 - accuracy: 0.9688 - val_loss: 0.3746 - val_accuracy: 0.9433\n",
            "Epoch 427/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.1220 - accuracy: 0.9720\n",
            "Epoch 427: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1248 - accuracy: 0.9714 - val_loss: 0.3366 - val_accuracy: 0.9399\n",
            "Epoch 428/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1249 - accuracy: 0.9739\n",
            "Epoch 428: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1240 - accuracy: 0.9729 - val_loss: 0.3408 - val_accuracy: 0.9410\n",
            "Epoch 429/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1135 - accuracy: 0.9756\n",
            "Epoch 429: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1128 - accuracy: 0.9757 - val_loss: 0.3408 - val_accuracy: 0.9473\n",
            "Epoch 430/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1033 - accuracy: 0.9784\n",
            "Epoch 430: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1013 - accuracy: 0.9777 - val_loss: 0.3524 - val_accuracy: 0.9445\n",
            "Epoch 431/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.0993 - accuracy: 0.9776\n",
            "Epoch 431: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0993 - accuracy: 0.9765 - val_loss: 0.3475 - val_accuracy: 0.9456\n",
            "Epoch 432/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.0905 - accuracy: 0.9771\n",
            "Epoch 432: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0894 - accuracy: 0.9770 - val_loss: 0.3493 - val_accuracy: 0.9416\n",
            "Epoch 433/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.0988 - accuracy: 0.9767\n",
            "Epoch 433: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0999 - accuracy: 0.9762 - val_loss: 0.3469 - val_accuracy: 0.9473\n",
            "Epoch 434/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1003 - accuracy: 0.9772\n",
            "Epoch 434: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1097 - accuracy: 0.9755 - val_loss: 0.3759 - val_accuracy: 0.9468\n",
            "Epoch 435/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.1346 - accuracy: 0.9696\n",
            "Epoch 435: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1345 - accuracy: 0.9701 - val_loss: 0.4220 - val_accuracy: 0.9445\n",
            "Epoch 436/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.1114 - accuracy: 0.9738\n",
            "Epoch 436: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1157 - accuracy: 0.9737 - val_loss: 0.3160 - val_accuracy: 0.9450\n",
            "Epoch 437/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1228 - accuracy: 0.9721\n",
            "Epoch 437: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1235 - accuracy: 0.9719 - val_loss: 0.3579 - val_accuracy: 0.9428\n",
            "Epoch 438/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1392 - accuracy: 0.9742\n",
            "Epoch 438: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1388 - accuracy: 0.9745 - val_loss: 0.3286 - val_accuracy: 0.9491\n",
            "Epoch 439/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1172 - accuracy: 0.9732\n",
            "Epoch 439: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1160 - accuracy: 0.9732 - val_loss: 0.3555 - val_accuracy: 0.9462\n",
            "Epoch 440/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1117 - accuracy: 0.9736\n",
            "Epoch 440: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1044 - accuracy: 0.9742 - val_loss: 0.3610 - val_accuracy: 0.9433\n",
            "Epoch 441/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.1159 - accuracy: 0.9747\n",
            "Epoch 441: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1166 - accuracy: 0.9749 - val_loss: 0.3070 - val_accuracy: 0.9445\n",
            "Epoch 442/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.1038 - accuracy: 0.9728\n",
            "Epoch 442: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.1019 - accuracy: 0.9731 - val_loss: 0.3222 - val_accuracy: 0.9428\n",
            "Epoch 443/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.1036 - accuracy: 0.9772\n",
            "Epoch 443: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1044 - accuracy: 0.9770 - val_loss: 0.3768 - val_accuracy: 0.9428\n",
            "Epoch 444/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.1074 - accuracy: 0.9719\n",
            "Epoch 444: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1059 - accuracy: 0.9714 - val_loss: 0.3647 - val_accuracy: 0.9445\n",
            "Epoch 445/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.0932 - accuracy: 0.9792\n",
            "Epoch 445: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0924 - accuracy: 0.9790 - val_loss: 0.3606 - val_accuracy: 0.9468\n",
            "Epoch 446/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.0846 - accuracy: 0.9793\n",
            "Epoch 446: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0906 - accuracy: 0.9784 - val_loss: 0.3847 - val_accuracy: 0.9450\n",
            "Epoch 447/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.1125 - accuracy: 0.9776\n",
            "Epoch 447: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1115 - accuracy: 0.9777 - val_loss: 0.3221 - val_accuracy: 0.9382\n",
            "Epoch 448/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.1216 - accuracy: 0.9759\n",
            "Epoch 448: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1194 - accuracy: 0.9754 - val_loss: 0.3252 - val_accuracy: 0.9405\n",
            "Epoch 449/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1299 - accuracy: 0.9699\n",
            "Epoch 449: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1271 - accuracy: 0.9709 - val_loss: 0.3130 - val_accuracy: 0.9485\n",
            "Epoch 450/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.1302 - accuracy: 0.9760\n",
            "Epoch 450: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1239 - accuracy: 0.9749 - val_loss: 0.3212 - val_accuracy: 0.9496\n",
            "Epoch 451/500\n",
            "50/55 [==========================>...] - ETA: 0s - loss: 0.1281 - accuracy: 0.9750\n",
            "Epoch 451: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1305 - accuracy: 0.9741 - val_loss: 0.2573 - val_accuracy: 0.9462\n",
            "Epoch 452/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.0943 - accuracy: 0.9762\n",
            "Epoch 452: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1038 - accuracy: 0.9742 - val_loss: 0.2652 - val_accuracy: 0.9445\n",
            "Epoch 453/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1002 - accuracy: 0.9751\n",
            "Epoch 453: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1008 - accuracy: 0.9747 - val_loss: 0.3102 - val_accuracy: 0.9456\n",
            "Epoch 454/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1295 - accuracy: 0.9718\n",
            "Epoch 454: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1362 - accuracy: 0.9724 - val_loss: 0.3541 - val_accuracy: 0.9439\n",
            "Epoch 455/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.1276 - accuracy: 0.9717\n",
            "Epoch 455: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1235 - accuracy: 0.9715 - val_loss: 0.3669 - val_accuracy: 0.9428\n",
            "Epoch 456/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.0991 - accuracy: 0.9757\n",
            "Epoch 456: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0995 - accuracy: 0.9752 - val_loss: 0.3555 - val_accuracy: 0.9473\n",
            "Epoch 457/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1089 - accuracy: 0.9764\n",
            "Epoch 457: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1097 - accuracy: 0.9759 - val_loss: 0.3562 - val_accuracy: 0.9450\n",
            "Epoch 458/500\n",
            "51/55 [==========================>...] - ETA: 0s - loss: 0.1081 - accuracy: 0.9726\n",
            "Epoch 458: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1048 - accuracy: 0.9731 - val_loss: 0.3409 - val_accuracy: 0.9439\n",
            "Epoch 459/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.0890 - accuracy: 0.9778\n",
            "Epoch 459: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0840 - accuracy: 0.9788 - val_loss: 0.3204 - val_accuracy: 0.9445\n",
            "Epoch 460/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1123 - accuracy: 0.9742\n",
            "Epoch 460: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1123 - accuracy: 0.9742 - val_loss: 0.3401 - val_accuracy: 0.9479\n",
            "Epoch 461/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1148 - accuracy: 0.9771\n",
            "Epoch 461: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1133 - accuracy: 0.9764 - val_loss: 0.3697 - val_accuracy: 0.9450\n",
            "Epoch 462/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1057 - accuracy: 0.9779\n",
            "Epoch 462: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1103 - accuracy: 0.9770 - val_loss: 0.3473 - val_accuracy: 0.9428\n",
            "Epoch 463/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1068 - accuracy: 0.9735\n",
            "Epoch 463: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1115 - accuracy: 0.9728 - val_loss: 0.3110 - val_accuracy: 0.9445\n",
            "Epoch 464/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1002 - accuracy: 0.9788\n",
            "Epoch 464: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1002 - accuracy: 0.9781 - val_loss: 0.3574 - val_accuracy: 0.9462\n",
            "Epoch 465/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.0792 - accuracy: 0.9806\n",
            "Epoch 465: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0814 - accuracy: 0.9802 - val_loss: 0.3831 - val_accuracy: 0.9388\n",
            "Epoch 466/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.0857 - accuracy: 0.9813\n",
            "Epoch 466: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0894 - accuracy: 0.9804 - val_loss: 0.3682 - val_accuracy: 0.9456\n",
            "Epoch 467/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1104 - accuracy: 0.9802\n",
            "Epoch 467: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1122 - accuracy: 0.9791 - val_loss: 0.3587 - val_accuracy: 0.9416\n",
            "Epoch 468/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1305 - accuracy: 0.9724\n",
            "Epoch 468: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1337 - accuracy: 0.9728 - val_loss: 0.3859 - val_accuracy: 0.9422\n",
            "Epoch 469/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1358 - accuracy: 0.9743\n",
            "Epoch 469: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1309 - accuracy: 0.9741 - val_loss: 0.3248 - val_accuracy: 0.9410\n",
            "Epoch 470/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1081 - accuracy: 0.9749\n",
            "Epoch 470: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1065 - accuracy: 0.9748 - val_loss: 0.3448 - val_accuracy: 0.9450\n",
            "Epoch 471/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.1027 - accuracy: 0.9753\n",
            "Epoch 471: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1034 - accuracy: 0.9745 - val_loss: 0.3577 - val_accuracy: 0.9445\n",
            "Epoch 472/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1023 - accuracy: 0.9742\n",
            "Epoch 472: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0992 - accuracy: 0.9749 - val_loss: 0.3608 - val_accuracy: 0.9450\n",
            "Epoch 473/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1058 - accuracy: 0.9746\n",
            "Epoch 473: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1029 - accuracy: 0.9751 - val_loss: 0.3415 - val_accuracy: 0.9422\n",
            "Epoch 474/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.1088 - accuracy: 0.9771\n",
            "Epoch 474: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.1087 - accuracy: 0.9771 - val_loss: 0.3601 - val_accuracy: 0.9365\n",
            "Epoch 475/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1425 - accuracy: 0.9707\n",
            "Epoch 475: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1376 - accuracy: 0.9715 - val_loss: 0.3444 - val_accuracy: 0.9393\n",
            "Epoch 476/500\n",
            "46/55 [========================>.....] - ETA: 0s - loss: 0.0973 - accuracy: 0.9766\n",
            "Epoch 476: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0979 - accuracy: 0.9765 - val_loss: 0.3372 - val_accuracy: 0.9456\n",
            "Epoch 477/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1310 - accuracy: 0.9738\n",
            "Epoch 477: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1240 - accuracy: 0.9747 - val_loss: 0.3690 - val_accuracy: 0.9353\n",
            "Epoch 478/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1630 - accuracy: 0.9691\n",
            "Epoch 478: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1570 - accuracy: 0.9694 - val_loss: 0.3735 - val_accuracy: 0.9365\n",
            "Epoch 479/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1354 - accuracy: 0.9727\n",
            "Epoch 479: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1304 - accuracy: 0.9731 - val_loss: 0.4239 - val_accuracy: 0.9405\n",
            "Epoch 480/500\n",
            "45/55 [=======================>......] - ETA: 0s - loss: 0.1114 - accuracy: 0.9733\n",
            "Epoch 480: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1074 - accuracy: 0.9738 - val_loss: 0.3452 - val_accuracy: 0.9428\n",
            "Epoch 481/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1157 - accuracy: 0.9712\n",
            "Epoch 481: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1194 - accuracy: 0.9705 - val_loss: 0.3532 - val_accuracy: 0.9433\n",
            "Epoch 482/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1280 - accuracy: 0.9702\n",
            "Epoch 482: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1280 - accuracy: 0.9702 - val_loss: 0.3597 - val_accuracy: 0.9468\n",
            "Epoch 483/500\n",
            "44/55 [=======================>......] - ETA: 0s - loss: 0.1448 - accuracy: 0.9719\n",
            "Epoch 483: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1496 - accuracy: 0.9708 - val_loss: 0.3710 - val_accuracy: 0.9485\n",
            "Epoch 484/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1100 - accuracy: 0.9732\n",
            "Epoch 484: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1165 - accuracy: 0.9738 - val_loss: 0.4142 - val_accuracy: 0.9456\n",
            "Epoch 485/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1458 - accuracy: 0.9701\n",
            "Epoch 485: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1451 - accuracy: 0.9696 - val_loss: 0.4110 - val_accuracy: 0.9473\n",
            "Epoch 486/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1142 - accuracy: 0.9727\n",
            "Epoch 486: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1082 - accuracy: 0.9735 - val_loss: 0.3734 - val_accuracy: 0.9456\n",
            "Epoch 487/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.1146 - accuracy: 0.9747\n",
            "Epoch 487: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.1142 - accuracy: 0.9745 - val_loss: 0.3997 - val_accuracy: 0.9450\n",
            "Epoch 488/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1013 - accuracy: 0.9762\n",
            "Epoch 488: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1017 - accuracy: 0.9749 - val_loss: 0.4453 - val_accuracy: 0.9462\n",
            "Epoch 489/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1324 - accuracy: 0.9764\n",
            "Epoch 489: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1309 - accuracy: 0.9761 - val_loss: 0.4241 - val_accuracy: 0.9416\n",
            "Epoch 490/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.1015 - accuracy: 0.9754\n",
            "Epoch 490: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1016 - accuracy: 0.9752 - val_loss: 0.4106 - val_accuracy: 0.9491\n",
            "Epoch 491/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1235 - accuracy: 0.9710\n",
            "Epoch 491: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.1187 - accuracy: 0.9722 - val_loss: 0.3991 - val_accuracy: 0.9456\n",
            "Epoch 492/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.0977 - accuracy: 0.9759\n",
            "Epoch 492: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0984 - accuracy: 0.9757 - val_loss: 0.4253 - val_accuracy: 0.9445\n",
            "Epoch 493/500\n",
            "47/55 [========================>.....] - ETA: 0s - loss: 0.0980 - accuracy: 0.9749\n",
            "Epoch 493: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0986 - accuracy: 0.9744 - val_loss: 0.4188 - val_accuracy: 0.9468\n",
            "Epoch 494/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1004 - accuracy: 0.9775\n",
            "Epoch 494: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 5ms/step - loss: 0.0972 - accuracy: 0.9774 - val_loss: 0.4477 - val_accuracy: 0.9399\n",
            "Epoch 495/500\n",
            "48/55 [=========================>....] - ETA: 0s - loss: 0.1091 - accuracy: 0.9774\n",
            "Epoch 495: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1185 - accuracy: 0.9775 - val_loss: 0.4002 - val_accuracy: 0.9422\n",
            "Epoch 496/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.1077 - accuracy: 0.9754\n",
            "Epoch 496: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.1069 - accuracy: 0.9755 - val_loss: 0.3893 - val_accuracy: 0.9393\n",
            "Epoch 497/500\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1113 - accuracy: 0.9744\n",
            "Epoch 497: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1113 - accuracy: 0.9744 - val_loss: 0.3642 - val_accuracy: 0.9473\n",
            "Epoch 498/500\n",
            "49/55 [=========================>....] - ETA: 0s - loss: 0.1168 - accuracy: 0.9746\n",
            "Epoch 498: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.1140 - accuracy: 0.9744 - val_loss: 0.3824 - val_accuracy: 0.9416\n",
            "Epoch 499/500\n",
            "54/55 [============================>.] - ETA: 0s - loss: 0.1039 - accuracy: 0.9760\n",
            "Epoch 499: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 7ms/step - loss: 0.1036 - accuracy: 0.9761 - val_loss: 0.3944 - val_accuracy: 0.9462\n",
            "Epoch 500/500\n",
            "52/55 [===========================>..] - ETA: 0s - loss: 0.0829 - accuracy: 0.9785\n",
            "Epoch 500: val_loss did not improve from 0.25094\n",
            "55/55 [==============================] - 0s 6ms/step - loss: 0.0862 - accuracy: 0.9791 - val_loss: 0.4055 - val_accuracy: 0.9491\n",
            "Training completed in time:  0:03:22.017682\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracy=model.evaluate(X_test,y_test,verbose=0)\n",
        "print(test_accuracy[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2q-L_b5dElC0",
        "outputId": "763a9270-4979-4d7b-dd6e-aa5175cbedbc"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.949055552482605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://freewavesamples.com/dog-bark download the sample\n",
        "filename=\"/content/drive/MyDrive/Proje/Dog-Bark.wav\"\n",
        "audio, sample_rate = librosa.load(filename, res_type='kaiser_fast') \n",
        "mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)"
      ],
      "metadata": {
        "id": "WU8HyzYNEngw"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mfccs_scaled_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pszdyqFNEpBv",
        "outputId": "8931bf80-2967-4e66-9afe-36c9b0033146"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40,)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mfccs_scaled_features=mfccs_scaled_features.reshape(1,-1)\n",
        "print(mfccs_scaled_features.shape)\n",
        "predicted_label = np.argmax(model.predict(mfccs_scaled_features), axis=-1)\n",
        "print('Predicted Label:',predicted_label)\n",
        "prediction_class = labelencoder.inverse_transform(predicted_label) \n",
        "prediction_class[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "XF8OwP2tEqLO",
        "outputId": "06898ec0-788e-410e-ba04-ccbb2f9ff16b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 40)\n",
            "Predicted Label: [3]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dog_bark'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(results.history['accuracy'])\n",
        "plt.plot(results.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "2rijXPu1ooeb",
        "outputId": "247ef357-c191-4220-a3de-1e55c45b7c8f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for loss\n",
        "plt.plot(results.history['loss'])\n",
        "plt.plot(results.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "nFuIqj-WosVj",
        "outputId": "953e482f-7bb3-4f5a-829d-c9203e3acc10"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FFzr7FvuqUYJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}